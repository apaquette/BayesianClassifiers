{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8bb9fa",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "1. [Introduction](#1)\n",
    "2. [Business Understanding](#2)\n",
    "3. [Data Understanding](#3)\n",
    "4. [Data Preperation](#4)\n",
    "5. [Modeling](#5)\n",
    "6. [Evaluation](#6)\n",
    "7. [Improvements](#7)\n",
    "8. [Conclusion](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a15ae22",
   "metadata": {},
   "source": [
    "# **1. Introduction** <a class=\"anchor\" id=\"1\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "This notebook will explore k Nearest Neighbour by going through the CRISP-DM process. The goal is to gain insight of how to use k-NN in Python and its usefulness. No other notebook was used, so there is no changelog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dcf4d5",
   "metadata": {},
   "source": [
    "# **2. Business Understanding** <a class=\"anchor\" id=\"2\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "The dataset I will be using for this notebook is the [Fracture Classification Dataset](https://www.kaggle.com/datasets/akshayramakrishnan28/fracture-classification-dataset/data). Specifically, I will be using the classified images for whether a scan depicts a fracture or non-fractured x-ray.\n",
    "\n",
    "Because the data is already classified, it should be easy to work with. In total there are a little over 4000 images, which will hopefully be enough to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ac64c",
   "metadata": {},
   "source": [
    "# **3. Data Understanding** <a class=\"anchor\" id=\"3\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Because we are working with images, we will have to make sure to flatten them when we store them for processing. Otherwise, our images are already labelled, making the data processing step easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This library will help us access our files\n",
    "try:\n",
    "    import os\n",
    "except:\n",
    "    !pip install os\n",
    "    import os\n",
    "\n",
    "# This library is for data processing, and CSV file I/O\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "# This library is for image processing\n",
    "try:\n",
    "    import cv2\n",
    "except:\n",
    "    !pip install opencv-python\n",
    "    import cv2\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "    \n",
    "def load_images(filepath, label):\n",
    "    images = []\n",
    "    for file in os.listdir(filepath):\n",
    "        image = cv2.imread(os.path.join(filepath,file))\n",
    "        image = cv2.resize(image, [128,128])\n",
    "        flattened_image = image.flatten() #flatten the image so that it is one-dimensional\n",
    "        images.append(flattened_image)\n",
    "    return np.array(images), [label] * len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af928d89",
   "metadata": {},
   "source": [
    "The code above takes the filepath to the images and iterates through them. using `cv2`, the images are read and resized to ensure they all take up the same size in the array. The images are then flattened so that they can be read by the machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61259241",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_working_directory = os.getcwd()\n",
    "fracturedFiles = os.path.join(current_working_directory, 'input', 'Fractured')\n",
    "nonFracturedFiles = os.path.join(current_working_directory, 'input', 'Non_fractured')\n",
    "\n",
    "Fracture_images, fracture_labels = load_images(fracturedFiles, 'Fractured')\n",
    "NonFracture_images, NonFracture_labels = load_images(nonFracturedFiles,'Non-Fractured')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'images': Fracture_images.tolist() + NonFracture_images.tolist(),\n",
    "    'fractured': np.concatenate([fracture_labels, NonFracture_labels])\n",
    "})\n",
    "\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0c26c",
   "metadata": {},
   "source": [
    "The code above uses the load_images method defined in the previous code block, and is provided with the filepath for the fractured images, and the non fractured images. The resulting arrays are combined together and added into a dataframe alongside the labels. The datarame is then shuffled to ensure the fractured and non fractured images are mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c82b12a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4083, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082eda72",
   "metadata": {},
   "source": [
    "Now that we've imported all of our data, we can see that we are working with 4083 images, and 2 attributes (images and fractured)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874da55d",
   "metadata": {},
   "source": [
    "# **4. Data Preperation** <a class=\"anchor\" id=\"4\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Because we are working with pre-categorized images, we know that there are no missing values. As such, we can jump straight into assigning our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beff65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetVariable = 'fractured'\n",
    "\n",
    "X = df.drop([targetVariable], axis=1)\n",
    "\n",
    "y = df[targetVariable]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3492f60",
   "metadata": {},
   "source": [
    "Next, we will split our data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11adc830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "X_train = X_train[\"images\"].tolist()\n",
    "X_test = X_test[\"images\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c0a34",
   "metadata": {},
   "source": [
    "# **5. Modeling** <a class=\"anchor\" id=\"5\"></a>\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e147f159",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_neighbors = 3 #the number of neighbors that will be checked for proximity\n",
    "knn_model = KNeighborsClassifier(k_neighbors)\n",
    "\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7217f8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Non-Fractured', 'Non-Fractured', 'Non-Fractured', ...,\n",
       "       'Non-Fractured', 'Non-Fractured', 'Fractured'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the results using the model\n",
    "y_pred = knn_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45d8d8",
   "metadata": {},
   "source": [
    "# **6. Evaluation** <a class=\"anchor\" id=\"6\"></a>\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b23047",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_train = knn_model.predict(X_train)\n",
    "    \n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896f620",
   "metadata": {},
   "source": [
    "The training-set accuracy score is 0.8220 while the test-set accuracy is 0.8898. The two values differ by about 8%, which is indicative of overfitting. This could be due to a low k value, which is the number of nearest neighbors used to determine what classification an image belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the most frequent value\n",
    "most_frequent = (y_test == \"Fractured\").sum() if (y_test == \"Fractured\").sum() >= (y_test == \"Non-Fractured\").sum() else (y_test == \"Non-Fractured\").sum()\n",
    "\n",
    "null_accuracy = most_frequent/len(y_test)\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fbd76",
   "metadata": {},
   "source": [
    "Ou model accuracy score is 0.8220 but our null accuracy is 0.8310. This means that our k-NN model is not doing a very good job predicting the classes, as it lower than the null accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f1b28",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
    "\n",
    "\n",
    "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
    "\n",
    "\n",
    "**True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
    "\n",
    "\n",
    "**True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
    "\n",
    "\n",
    "**False Positives (FP)** – False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n",
    "\n",
    "\n",
    "\n",
    "**False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n",
    "\n",
    "\n",
    "\n",
    "These four outcomes are summarized in a confusion matrix given below.\n",
    "\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259e817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefc9806",
   "metadata": {},
   "source": [
    "The confusion matrix shows `52 + 948 = 100 correct predictions` and `175 + 50 = 225 incorrect predictions`.\n",
    "\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "\n",
    "- `True Positives` (Actual Positive:1 and Predict Positive:1) - 52\n",
    "\n",
    "\n",
    "- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 948\n",
    "\n",
    "\n",
    "- `False Positives` (Actual Negative:0 but Predict Positive:1) - 175 `(Type I error)`\n",
    "\n",
    "\n",
    "- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 50 `(Type II error)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8eb7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "try:\n",
    "    import seaborn as sns # for statistical data visualization\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns # for statistical data visualization\n",
    "\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Yes', 'Actual No'], \n",
    "                                 index=['Predict Yes', 'Predict No'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09794848",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "\n",
    "**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model.\n",
    "\n",
    "We can print a classification report as follows:-\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c123e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import classification_report\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f225ca",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "**Accuracy** measures the overall correctness of the predictions. It is a general indicator of how well the model is performing.\n",
    "\n",
    "Mathematically, accuracy can be defined as the ratio of `(TP + FP) to (TP + FP + FN + TN)`\n",
    "\n",
    "### Precision\n",
    "\n",
    "\n",
    "**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). \n",
    "\n",
    "\n",
    "So, **Precision** identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
    "\n",
    "\n",
    "\n",
    "Mathematically, precision can be defined as the ratio of `TP to (TP + FP)`.\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)\n",
    "\n",
    "### Recall\n",
    "\n",
    "\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.\n",
    "It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.\n",
    "\n",
    "\n",
    "**Recall** identifies the proportion of correctly predicted actual positives.\n",
    "\n",
    "\n",
    "Mathematically, recall can be given as the ratio of `TP to (TP + FN)`.\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)\n",
    "\n",
    "### Specificity\n",
    "\n",
    "**Specificity** represents the propertion of correctly identified actual negatives. It helps us understand how well the model can idenitfy instances that don't belong the the positive class. High specificity is indicative that the model is good at avoiding false positives. Low specificity is indicative that the model is misclassifying negative instances as positive.\n",
    "\n",
    "Mathematically, specificity can be given as the ratio of `TN to (TN + FP)`\n",
    "\n",
    "### Negative Predictive Value (NPV)\n",
    "**NPV** asses the likelihood that a negative prediction is correct. High NPV indicates the model is correctly idenitfying true negatives. Low NPV indicates the model is missclassifying negative instances.\n",
    "\n",
    "Mathematically, NPV can be given as the ratio of `TN to (TN + FN)`\n",
    "\n",
    "### f1-score\n",
    "\n",
    "\n",
    "**f1-score** is the weighted harmonic mean of precision and recall. The best possible **f1-score** would be 1.0 and the worst \n",
    "would be 0.0.  **f1-score** is the harmonic mean of precision and recall. So, **f1-score** is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of `f1-score` should be used to \n",
    "compare classifier models, not global accuracy.\n",
    "\n",
    "Mathematically, f1-score can be given by the following formula: `2 x (Percision x Recall)/(Precision + Recall)`\n",
    "\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bf51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "negative_predictive_value = TN / (TN + FN)\n",
    "accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "f1_score = 2*((precision * recall) / (precision + recall))\n",
    "balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "print('Accuracy : {0:0.4f}'.format(accuracy))\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "print('Recall: {0:0.4f}'.format(recall))\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "print('NPV: {0:0.4f}'.format(negative_predictive_value))\n",
    "print('F1 Score: {0:0.4f}'.format(f1_score))\n",
    "print('Balanced Accuracy: {0:0.4f}'.format(balanced_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80502711",
   "metadata": {},
   "source": [
    "# **7. Improvements** <a class=\"anchor\" id=\"7\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Now that we've evaluated our model, we can reflect on our result and see if there are any improvements to be made.\n",
    "\n",
    "At first glance, a `82.2%` accuracy may seem promising. However, our precision, recall, and F1 Score is quite low. We can see from our confusion matrix that our model is quite good at predicting non-fracture images. However, is it not very good at identifying fractures, which is concerning since we want our model to be good at identifying fractures first and foremost. This is likely because our dataset does not contain enough images with fractures (717 out of a total of 4083, only 17%!!!). To solve this, we can sample our dataset to train on an equal amount of fractured and non-fractured images. Alternatively, we can try to obtain a higher amount of fractured images, which would require additional data collection. \n",
    "\n",
    "Another problem may be related to the resizing of the images. Many of the images are 2000 pixels in the x and y direction, and they are being resized to 128x128. There is likely a lot of detail being lost in the image, hindering the model's ability to properly identify fractures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99f162",
   "metadata": {},
   "source": [
    "To improve our model, we will trying increasing the amount of pixels the images are resized to.\n",
    "\n",
    "### Hardware Limitations\n",
    "When working with large datasets, there may be hardware limitations when loading data into memory. In this case, the size of the image is such that large amounts of memory is needed. In my case, I was able to load the images at size 700x700, but 1000x1000 would cause a memory error. My machine has 16GB of memory, so you may need to modfiy the pixels based on your hardware. Ideally we'd want as high a resolution as possible, as this results in more data for our model to train on, resulting in a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ad614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(filepath, label):\n",
    "    images = []\n",
    "    for file in os.listdir(filepath):\n",
    "        image = cv2.imread(os.path.join(filepath,file))\n",
    "        image = cv2.resize(image, [700,700])\n",
    "        flattened_image = image.flatten() #flatten the image so that it is one-dimensional\n",
    "        images.append(flattened_image)\n",
    "        if(len(images) == 717):\n",
    "            break\n",
    "    return np.array(images), [label] * len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a122f57",
   "metadata": {},
   "source": [
    "Next, we will attempt to pull an even amount of fractured and non-fractured images. Because our dataset comprises of mostly non-fractured images, we can try and pull an even amount of images as the count of fractured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fracture_images, fracture_labels = load_images(fracturedFiles, 'Fractured')\n",
    "NonFracture_images, NonFracture_labels = load_images(nonFracturedFiles,'Non-Fractured')\n",
    "\n",
    "Fracture_images.shape, NonFracture_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2101b789",
   "metadata": {},
   "source": [
    "Now using our modified load_images method, we have an equal amount of fractured and non-fractured images. The images are also bigger, which will hopefully improve our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allImages = Fracture_images.tolist() + NonFracture_images.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71766bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'images': allImages,\n",
    "    'fractured': np.concatenate([fracture_labels, NonFracture_labels])\n",
    "})\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4e0efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetVariable = 'fractured'\n",
    "\n",
    "X = df.drop([targetVariable], axis=1)\n",
    "\n",
    "y = df[targetVariable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 0)\n",
    "X_train = X_train[\"images\"].tolist()\n",
    "X_test = X_test[\"images\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_neighbors = 7 #the number of neighbors that will be checked for proximity\n",
    "knn_model = KNeighborsClassifier(k_neighbors)\n",
    "\n",
    "knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d37da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the results using the model\n",
    "y_pred = knn_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_train = knn_model.predict(X_train)\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the most frequent value\n",
    "most_frequent = (y_test == \"Fractured\").sum() if (y_test == \"Fractured\").sum() >= (y_test == \"Non-Fractured\").sum() else (y_test == \"Non-Fractured\").sum()\n",
    "\n",
    "null_accuracy = most_frequent/len(y_test)\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d36bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "try:\n",
    "    import seaborn as sns # for statistical data visualization\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns # for statistical data visualization\n",
    "\n",
    "\n",
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Yes', 'Actual No'], \n",
    "                                 index=['Predict Yes', 'Predict No'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.metrics import classification_report\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e3147",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "negative_predictive_value = TN / (TN + FN)\n",
    "accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "f1_score = 2*((precision * recall) / (precision + recall))\n",
    "balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "print('Accuracy : {0:0.4f}'.format(accuracy))\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "print('Recall: {0:0.4f}'.format(recall))\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "print('NPV: {0:0.4f}'.format(negative_predictive_value))\n",
    "print('F1 Score: {0:0.4f}'.format(f1_score))\n",
    "print('Balanced Accuracy: {0:0.4f}'.format(balanced_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508fd6b",
   "metadata": {},
   "source": [
    "Looking at our results, at first glance, it may seem our new model is worse than our previous one. Our accuracy is reduced to 68% vs 83%. However, our new model is better trained overall, with a massive increase in precision by 45%, and all other metrics higher overall. Our model accuracy is also much higher than our null accuracy.\n",
    "\n",
    "In spite of all of this, 68% is still too low to be usable. To improve our model, we would likely need one of two things: more images, higher pixel count, and a higher k-value. However, we are currently running into RAM limitations, so further improvement of the model by increasing the dataset is not feasible without better hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6778af3",
   "metadata": {},
   "source": [
    "# **8. Conclusion** <a class=\"anchor\" id=\"8\"></a>\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d66f4e",
   "metadata": {},
   "source": [
    "Looking back on what we've acheived in this notebook, we've successfully gone through the phases of CRISP-DM to generate a machine learning model that can predict the presence of fractures in X-Rays.\n",
    "1. `Business Understanding`: This phase helped us understand what our data represents so we could make better understand how to train our model to make predictions.\n",
    "2. `Data Understanding`: This phase showed us what type of data we were working with, and how to organize it to train our model.\n",
    "3. `Data Preperation`: This step allowed us to pull our images and resize them to an acceptable size that would work within our hardware limitations. We also combined our two datasets and labelled them.\n",
    "4. `Modeling`: In this step we were able to use our training set to train the model, and validate it using the testing set.\n",
    "5. `Evaluation`: In this step, we were able to identify a major issue with our model. That being that despite its high accuracy score, it was unable to succesfully identify fractures. This is because of the over-representation of non-fractured images.\n",
    "6. `Improvements`: After identifying the over-representation issue, we re-structured our dataset to have an equal number of images in both categories. We ran into hardware limitations related to loading the data into memory. As such, we had to find the maximum number of pixels that could be loaded without causing memory exceptions. Our resulting model is much improved from our previous one, but we are limited by hardware to push it further from where it is now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
