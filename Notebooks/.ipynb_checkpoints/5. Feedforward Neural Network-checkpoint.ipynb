{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772987a3",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Table of Contents**\n",
    "1. [Introduction](#1)\n",
    "2. [Business Understanding](#2)\n",
    "3. [Data Understanding](#3)\n",
    "4. [Data Preperation](#4)\n",
    "5. [Modeling](#5)\n",
    "6. [Evaluation](#6)\n",
    "7. [Improvements](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1936fbce",
   "metadata": {},
   "source": [
    "# **1. Introduction** <a class=\"anchor\" id=\"1\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "This notebook will explore Feedforward Neural Networks by going through the CRISP-DM process. The goal is to gain insight into how to use FNN in Python.\n",
    "\n",
    "### Feedforward Neural Networks\n",
    "\n",
    "Feedforward Neural Networks (FNNs) are a type of neural network where the connection between nodes do not form any cycle. Information moves forward, from the input nodes, through the hidden nodes, to the output nodes. The following is the typical process of a FNN:\n",
    "\n",
    "**1. Forward Pass**\n",
    "\n",
    "Input data is fed into the input layer. Each neuron passes its signal forward to all neurons in the hidden layer. Each connection between neurons has an associated weight that determines the strength of the connection.\n",
    "\n",
    "**2. Activation Function**\n",
    "\n",
    "At each neuron in the hidden layer, the weighed sum of inputs is calculated, and then an activation function is applied. Common activation functions include sigmoid, tanh, and ReLU (Rectified Linear Unit).\n",
    "\n",
    "**3. Propagation**\n",
    "\n",
    "The output of each neuron becomes the input of the neurons in the next layer. This process continues until the output layer is reached.\n",
    "\n",
    "**4. Output**\n",
    "\n",
    "The output layer represents the predictions or classifications made by the neural network based on the input data.\n",
    "\n",
    "**5. Error Calculation and Backpropagation**\n",
    "\n",
    "The network's output is compared to the actual actual target value, and the error is calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea85272",
   "metadata": {},
   "source": [
    "# **2. Business Understanding** <a class=\"anchor\" id=\"2\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "The dataset I will be using for this notebook is the [Phishing Url](https://www.kaggle.com/datasets/hemanthpingali/phishing-url?resource=download). I will be attempting to train a neural network to identify phishing urls.\n",
    "\n",
    "The dataset contains various features extracted from the urls including length, presence of elements (IP, TLDs), content-related attributes, technical indicators, and others. Altogether there are 87 such attributes in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88947a9",
   "metadata": {},
   "source": [
    "# **3. Data Understanding** <a class=\"anchor\" id=\"3\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "We will now explore our dataset to understand the data we are working with. Our data is stored in a .parquet file, so we'll want to convert it to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e665843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# This library will help us access our file\n",
    "try:\n",
    "    import os\n",
    "except:\n",
    "    !pip install os\n",
    "    import os\n",
    "    \n",
    "# This library is for data processing, and file I/O\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "except:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "# model training libraries\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "except:\n",
    "    !pip install tensorflow\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense    \n",
    "    \n",
    "# Eval libraries\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "try:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import classification_report\n",
    "except:\n",
    "    !pip install sklearn\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "# visualize confusion matrix with seaborn heatmap\n",
    "try:\n",
    "    import seaborn as sns # for statistical data visualization\n",
    "except:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns # for statistical data visualization\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dea5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_working_directory = os.getcwd()\n",
    "file = r'\\input\\phishingURL.parquet' #this directory may differ for you\n",
    "data = current_working_directory + file\n",
    "\n",
    "# convert to a csv\n",
    "df = pd.read_parquet(data)\n",
    "df.to_csv(current_working_directory + r'\\input\\phishingURL.csv') \n",
    "\n",
    "# read the new csv\n",
    "data = current_working_directory + r'\\input\\phishingURL.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc5dd2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3772, 90)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838ad3b",
   "metadata": {},
   "source": [
    "We can see that there are 3772 instances, and 90 attributes, which is more than what we were expecting from business understanding. Let's look at the columns and determine what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8ae496",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>ip</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>domain_with_copyright</th>\n",
       "      <th>whois_registered_domain</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://clubedemilhagem.com/home.php</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.medicalnewstoday.com/articles/18893...</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>6106</td>\n",
       "      <td>737</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://en.wikipedia.org/wiki/NBC_Nightly_News</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>7134</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://secure.web894.com/customer_center/custo...</td>\n",
       "      <td>185</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Transaction_proc...</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>7134</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                url  length_url  \\\n",
       "0           0               https://clubedemilhagem.com/home.php          36   \n",
       "1           1  http://www.medicalnewstoday.com/articles/18893...          51   \n",
       "2           2     https://en.wikipedia.org/wiki/NBC_Nightly_News          46   \n",
       "3           3  http://secure.web894.com/customer_center/custo...         185   \n",
       "4           4  https://en.wikipedia.org/wiki/Transaction_proc...          52   \n",
       "\n",
       "   length_hostname  ip  nb_dots  nb_hyphens  nb_at  nb_qm  nb_and  ...  \\\n",
       "0               19   0        2           0      0      0       0  ...   \n",
       "1               24   0        3           0      0      0       0  ...   \n",
       "2               16   0        2           0      0      0       0  ...   \n",
       "3               17   1        2           1      0      1       2  ...   \n",
       "4               16   0        2           0      0      0       0  ...   \n",
       "\n",
       "   domain_in_title  domain_with_copyright  whois_registered_domain  \\\n",
       "0                1                      0                        0   \n",
       "1                1                      1                        0   \n",
       "2                0                      1                        0   \n",
       "3                1                      1                        0   \n",
       "4                0                      1                        0   \n",
       "\n",
       "   domain_registration_length  domain_age  web_traffic  dns_record  \\\n",
       "0                         344          21            0           0   \n",
       "1                         103        6106          737           0   \n",
       "2                         901        7134           12           0   \n",
       "3                         247        1944            0           0   \n",
       "4                         901        7134           12           0   \n",
       "\n",
       "   google_index  page_rank      status  \n",
       "0             1          0    phishing  \n",
       "1             1          6  legitimate  \n",
       "2             0          7  legitimate  \n",
       "3             1          0    phishing  \n",
       "4             0          7  legitimate  \n",
       "\n",
       "[5 rows x 90 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b57b33",
   "metadata": {},
   "source": [
    "Here we can see that from our 90 columns, two are indices (first and second), and the third is the url, meaning that the remaining 87 columns match our expected attributes. We can drop the extra indice column during our data preperation phase, since they won't be necessary to train our model. We can also drop the url column, as the rest of the attributes are data about the url, which is what we want to use to make our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b8004d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3772 entries, 0 to 3771\n",
      "Data columns (total 90 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   Unnamed: 0                  3772 non-null   int64  \n",
      " 1   url                         3772 non-null   object \n",
      " 2   length_url                  3772 non-null   int64  \n",
      " 3   length_hostname             3772 non-null   int64  \n",
      " 4   ip                          3772 non-null   int64  \n",
      " 5   nb_dots                     3772 non-null   int64  \n",
      " 6   nb_hyphens                  3772 non-null   int64  \n",
      " 7   nb_at                       3772 non-null   int64  \n",
      " 8   nb_qm                       3772 non-null   int64  \n",
      " 9   nb_and                      3772 non-null   int64  \n",
      " 10  nb_or                       3772 non-null   int64  \n",
      " 11  nb_eq                       3772 non-null   int64  \n",
      " 12  nb_underscore               3772 non-null   int64  \n",
      " 13  nb_tilde                    3772 non-null   int64  \n",
      " 14  nb_percent                  3772 non-null   int64  \n",
      " 15  nb_slash                    3772 non-null   int64  \n",
      " 16  nb_star                     3772 non-null   int64  \n",
      " 17  nb_colon                    3772 non-null   int64  \n",
      " 18  nb_comma                    3772 non-null   int64  \n",
      " 19  nb_semicolumn               3772 non-null   int64  \n",
      " 20  nb_dollar                   3772 non-null   int64  \n",
      " 21  nb_space                    3772 non-null   int64  \n",
      " 22  nb_www                      3772 non-null   int64  \n",
      " 23  nb_com                      3772 non-null   int64  \n",
      " 24  nb_dslash                   3772 non-null   int64  \n",
      " 25  http_in_path                3772 non-null   int64  \n",
      " 26  https_token                 3772 non-null   int64  \n",
      " 27  ratio_digits_url            3772 non-null   float64\n",
      " 28  ratio_digits_host           3772 non-null   float64\n",
      " 29  punycode                    3772 non-null   int64  \n",
      " 30  port                        3772 non-null   int64  \n",
      " 31  tld_in_path                 3772 non-null   int64  \n",
      " 32  tld_in_subdomain            3772 non-null   int64  \n",
      " 33  abnormal_subdomain          3772 non-null   int64  \n",
      " 34  nb_subdomains               3772 non-null   int64  \n",
      " 35  prefix_suffix               3772 non-null   int64  \n",
      " 36  random_domain               3772 non-null   int64  \n",
      " 37  shortening_service          3772 non-null   int64  \n",
      " 38  path_extension              3772 non-null   int64  \n",
      " 39  nb_redirection              3772 non-null   int64  \n",
      " 40  nb_external_redirection     3772 non-null   int64  \n",
      " 41  length_words_raw            3772 non-null   int64  \n",
      " 42  char_repeat                 3772 non-null   int64  \n",
      " 43  shortest_words_raw          3772 non-null   int64  \n",
      " 44  shortest_word_host          3772 non-null   int64  \n",
      " 45  shortest_word_path          3772 non-null   int64  \n",
      " 46  longest_words_raw           3772 non-null   int64  \n",
      " 47  longest_word_host           3772 non-null   int64  \n",
      " 48  longest_word_path           3772 non-null   int64  \n",
      " 49  avg_words_raw               3772 non-null   float64\n",
      " 50  avg_word_host               3772 non-null   float64\n",
      " 51  avg_word_path               3772 non-null   float64\n",
      " 52  phish_hints                 3772 non-null   int64  \n",
      " 53  domain_in_brand             3772 non-null   int64  \n",
      " 54  brand_in_subdomain          3772 non-null   int64  \n",
      " 55  brand_in_path               3772 non-null   int64  \n",
      " 56  suspecious_tld              3772 non-null   int64  \n",
      " 57  statistical_report          3772 non-null   int64  \n",
      " 58  nb_hyperlinks               3772 non-null   int64  \n",
      " 59  ratio_intHyperlinks         3772 non-null   float64\n",
      " 60  ratio_extHyperlinks         3772 non-null   float64\n",
      " 61  ratio_nullHyperlinks        3772 non-null   int64  \n",
      " 62  nb_extCSS                   3772 non-null   int64  \n",
      " 63  ratio_intRedirection        3772 non-null   int64  \n",
      " 64  ratio_extRedirection        3772 non-null   float64\n",
      " 65  ratio_intErrors             3772 non-null   int64  \n",
      " 66  ratio_extErrors             3772 non-null   float64\n",
      " 67  login_form                  3772 non-null   int64  \n",
      " 68  external_favicon            3772 non-null   int64  \n",
      " 69  links_in_tags               3772 non-null   float64\n",
      " 70  submit_email                3772 non-null   int64  \n",
      " 71  ratio_intMedia              3772 non-null   float64\n",
      " 72  ratio_extMedia              3772 non-null   float64\n",
      " 73  sfh                         3772 non-null   int64  \n",
      " 74  iframe                      3772 non-null   int64  \n",
      " 75  popup_window                3772 non-null   int64  \n",
      " 76  safe_anchor                 3772 non-null   float64\n",
      " 77  onmouseover                 3772 non-null   int64  \n",
      " 78  right_clic                  3772 non-null   int64  \n",
      " 79  empty_title                 3772 non-null   int64  \n",
      " 80  domain_in_title             3772 non-null   int64  \n",
      " 81  domain_with_copyright       3772 non-null   int64  \n",
      " 82  whois_registered_domain     3772 non-null   int64  \n",
      " 83  domain_registration_length  3772 non-null   int64  \n",
      " 84  domain_age                  3772 non-null   int64  \n",
      " 85  web_traffic                 3772 non-null   int64  \n",
      " 86  dns_record                  3772 non-null   int64  \n",
      " 87  google_index                3772 non-null   int64  \n",
      " 88  page_rank                   3772 non-null   int64  \n",
      " 89  status                      3772 non-null   object \n",
      "dtypes: float64(13), int64(75), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0db07e",
   "metadata": {},
   "source": [
    "Here we can see that we have no missing values for our 3772 records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6af70",
   "metadata": {},
   "source": [
    "### Explore Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b19bfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 categorical variables\n",
      "\n",
      "The categorical variables are: ['url', 'status']\n"
     ]
    }
   ],
   "source": [
    "# identify categorical variables\n",
    "\n",
    "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
    "\n",
    "print(f'There are {len(categorical)} categorical variables\\n')\n",
    "print(f'The categorical variables are: {categorical}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002a03b",
   "metadata": {},
   "source": [
    "### Explore Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7275f6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 88 numerical variables\n",
      "\n",
      "The numerical variables are: ['Unnamed: 0', 'length_url', 'length_hostname', 'ip', 'nb_dots', 'nb_hyphens', 'nb_at', 'nb_qm', 'nb_and', 'nb_or', 'nb_eq', 'nb_underscore', 'nb_tilde', 'nb_percent', 'nb_slash', 'nb_star', 'nb_colon', 'nb_comma', 'nb_semicolumn', 'nb_dollar', 'nb_space', 'nb_www', 'nb_com', 'nb_dslash', 'http_in_path', 'https_token', 'ratio_digits_url', 'ratio_digits_host', 'punycode', 'port', 'tld_in_path', 'tld_in_subdomain', 'abnormal_subdomain', 'nb_subdomains', 'prefix_suffix', 'random_domain', 'shortening_service', 'path_extension', 'nb_redirection', 'nb_external_redirection', 'length_words_raw', 'char_repeat', 'shortest_words_raw', 'shortest_word_host', 'shortest_word_path', 'longest_words_raw', 'longest_word_host', 'longest_word_path', 'avg_words_raw', 'avg_word_host', 'avg_word_path', 'phish_hints', 'domain_in_brand', 'brand_in_subdomain', 'brand_in_path', 'suspecious_tld', 'statistical_report', 'nb_hyperlinks', 'ratio_intHyperlinks', 'ratio_extHyperlinks', 'ratio_nullHyperlinks', 'nb_extCSS', 'ratio_intRedirection', 'ratio_extRedirection', 'ratio_intErrors', 'ratio_extErrors', 'login_form', 'external_favicon', 'links_in_tags', 'submit_email', 'ratio_intMedia', 'ratio_extMedia', 'sfh', 'iframe', 'popup_window', 'safe_anchor', 'onmouseover', 'right_clic', 'empty_title', 'domain_in_title', 'domain_with_copyright', 'whois_registered_domain', 'domain_registration_length', 'domain_age', 'web_traffic', 'dns_record', 'google_index', 'page_rank']\n"
     ]
    }
   ],
   "source": [
    "# identify numerical variables\n",
    "\n",
    "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
    "\n",
    "print(f'There are {len(numerical)} numerical variables\\n')\n",
    "print(f'The numerical variables are: {numerical}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c18d12",
   "metadata": {},
   "source": [
    "# **4. Data Preperation** <a class=\"anchor\" id=\"4\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "During the Data Understanding phase, we noticed there were some unecessary columns in our dataset. We will drop the first two columns, as they won't helpful for us to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0484c24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>ip</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>domain_with_copyright</th>\n",
       "      <th>whois_registered_domain</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>6106</td>\n",
       "      <td>737</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>7134</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>7134</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_url  length_hostname  ip  nb_dots  nb_hyphens  nb_at  nb_qm  nb_and  \\\n",
       "0          36               19   0        2           0      0      0       0   \n",
       "1          51               24   0        3           0      0      0       0   \n",
       "2          46               16   0        2           0      0      0       0   \n",
       "3         185               17   1        2           1      0      1       2   \n",
       "4          52               16   0        2           0      0      0       0   \n",
       "\n",
       "   nb_or  nb_eq  ...  domain_in_title  domain_with_copyright  \\\n",
       "0      0      0  ...                1                      0   \n",
       "1      0      0  ...                1                      1   \n",
       "2      0      0  ...                0                      1   \n",
       "3      0      3  ...                1                      1   \n",
       "4      0      0  ...                0                      1   \n",
       "\n",
       "   whois_registered_domain  domain_registration_length  domain_age  \\\n",
       "0                        0                         344          21   \n",
       "1                        0                         103        6106   \n",
       "2                        0                         901        7134   \n",
       "3                        0                         247        1944   \n",
       "4                        0                         901        7134   \n",
       "\n",
       "   web_traffic  dns_record  google_index  page_rank      status  \n",
       "0            0           0             1          0    phishing  \n",
       "1          737           0             1          6  legitimate  \n",
       "2           12           0             0          7  legitimate  \n",
       "3            0           0             1          0    phishing  \n",
       "4           12           0             0          7  legitimate  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.columns[:2], inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a065392",
   "metadata": {},
   "source": [
    "Now we want to extract our target variable from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca1e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'status'\n",
    "\n",
    "X_target = df.drop([target_variable], axis=1)\n",
    "\n",
    "y = df[target_variable]\n",
    "df.drop([target_variable], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f3a01",
   "metadata": {},
   "source": [
    "Next we want to scale our values to ensure all attributes contribute equally to the model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977bd01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>ip</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>empty_title</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>domain_with_copyright</th>\n",
       "      <th>whois_registered_domain</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>6106</td>\n",
       "      <td>737</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>7134</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>1944</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>7134</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373</td>\n",
       "      <td>6202</td>\n",
       "      <td>7701846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>6071</td>\n",
       "      <td>14420</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "      <td>5971</td>\n",
       "      <td>402341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>550</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "      <td>6591</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>77</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>5767</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3772 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length_url  length_hostname  ip  nb_dots  nb_hyphens  nb_at  nb_qm  \\\n",
       "0             36               19   0        2           0      0      0   \n",
       "1             51               24   0        3           0      0      0   \n",
       "2             46               16   0        2           0      0      0   \n",
       "3            185               17   1        2           1      0      1   \n",
       "4             52               16   0        2           0      0      0   \n",
       "...          ...              ...  ..      ...         ...    ...    ...   \n",
       "3767          34               26   0        2           0      0      0   \n",
       "3768          54               14   0        2           0      0      0   \n",
       "3769          25               16   0        2           0      0      0   \n",
       "3770         550               25   1        5          24      0      1   \n",
       "3771          77               23   1        1           0      0      0   \n",
       "\n",
       "      nb_and  nb_or  nb_eq  ...  empty_title  domain_in_title  \\\n",
       "0          0      0      0  ...            0                1   \n",
       "1          0      0      0  ...            0                1   \n",
       "2          0      0      0  ...            0                0   \n",
       "3          2      0      3  ...            0                1   \n",
       "4          0      0      0  ...            0                0   \n",
       "...      ...    ...    ...  ...          ...              ...   \n",
       "3767       0      0      0  ...            0                0   \n",
       "3768       0      0      0  ...            0                1   \n",
       "3769       0      0      0  ...            0                0   \n",
       "3770       9      0     10  ...            0                1   \n",
       "3771       0      0      0  ...            0                1   \n",
       "\n",
       "      domain_with_copyright  whois_registered_domain  \\\n",
       "0                         0                        0   \n",
       "1                         1                        0   \n",
       "2                         1                        0   \n",
       "3                         1                        0   \n",
       "4                         1                        0   \n",
       "...                     ...                      ...   \n",
       "3767                      0                        0   \n",
       "3768                      1                        0   \n",
       "3769                      0                        0   \n",
       "3770                      1                        0   \n",
       "3771                      0                        0   \n",
       "\n",
       "      domain_registration_length  domain_age  web_traffic  dns_record  \\\n",
       "0                            344          21            0           0   \n",
       "1                            103        6106          737           0   \n",
       "2                            901        7134           12           0   \n",
       "3                            247        1944            0           0   \n",
       "4                            901        7134           12           0   \n",
       "...                          ...         ...          ...         ...   \n",
       "3767                         373        6202      7701846           0   \n",
       "3768                         139        6071        14420           0   \n",
       "3769                         238        5971       402341           0   \n",
       "3770                         349        6591           30           0   \n",
       "3771                          77        5767            0           0   \n",
       "\n",
       "      google_index  page_rank  \n",
       "0                1          0  \n",
       "1                1          6  \n",
       "2                0          7  \n",
       "3                1          0  \n",
       "4                0          7  \n",
       "...            ...        ...  \n",
       "3767             0          5  \n",
       "3768             0          5  \n",
       "3769             0          3  \n",
       "3770             1          4  \n",
       "3771             1          2  \n",
       "\n",
       "[3772 rows x 87 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f459933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>ip</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>...</th>\n",
       "      <th>empty_title</th>\n",
       "      <th>domain_in_title</th>\n",
       "      <th>domain_with_copyright</th>\n",
       "      <th>whois_registered_domain</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>domain_age</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>google_index</th>\n",
       "      <th>page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.530043</td>\n",
       "      <td>-0.192143</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.37083</td>\n",
       "      <td>0.545898</td>\n",
       "      <td>-0.890858</td>\n",
       "      <td>-0.27377</td>\n",
       "      <td>-0.200046</td>\n",
       "      <td>-1.303867</td>\n",
       "      <td>-0.427241</td>\n",
       "      <td>-0.135494</td>\n",
       "      <td>0.917005</td>\n",
       "      <td>-1.255602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.216023</td>\n",
       "      <td>0.229918</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>0.365720</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.37083</td>\n",
       "      <td>0.545898</td>\n",
       "      <td>1.122513</td>\n",
       "      <td>-0.27377</td>\n",
       "      <td>-0.538596</td>\n",
       "      <td>0.650155</td>\n",
       "      <td>-0.426863</td>\n",
       "      <td>-0.135494</td>\n",
       "      <td>0.917005</td>\n",
       "      <td>1.119788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.320696</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.37083</td>\n",
       "      <td>-1.831846</td>\n",
       "      <td>1.122513</td>\n",
       "      <td>-0.27377</td>\n",
       "      <td>0.582413</td>\n",
       "      <td>0.980268</td>\n",
       "      <td>-0.427235</td>\n",
       "      <td>-0.135494</td>\n",
       "      <td>-1.090507</td>\n",
       "      <td>1.515686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.589220</td>\n",
       "      <td>-0.360967</td>\n",
       "      <td>2.331708</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>2.391141</td>\n",
       "      <td>2.310396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.843696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.37083</td>\n",
       "      <td>0.545898</td>\n",
       "      <td>1.122513</td>\n",
       "      <td>-0.27377</td>\n",
       "      <td>-0.336309</td>\n",
       "      <td>-0.686351</td>\n",
       "      <td>-0.427241</td>\n",
       "      <td>-0.135494</td>\n",
       "      <td>0.917005</td>\n",
       "      <td>-1.255602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.195088</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.428870</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.37083</td>\n",
       "      <td>-1.831846</td>\n",
       "      <td>1.122513</td>\n",
       "      <td>-0.27377</td>\n",
       "      <td>0.582413</td>\n",
       "      <td>0.980268</td>\n",
       "      <td>-0.427235</td>\n",
       "      <td>-0.135494</td>\n",
       "      <td>-1.090507</td>\n",
       "      <td>1.515686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   length_url  length_hostname        ip   nb_dots  nb_hyphens     nb_at  \\\n",
       "0   -0.530043        -0.192143 -0.428870 -0.374353   -0.458789 -0.151506   \n",
       "1   -0.216023         0.229918 -0.428870  0.365720   -0.458789 -0.151506   \n",
       "2   -0.320696        -0.445380 -0.428870 -0.374353   -0.458789 -0.151506   \n",
       "3    2.589220        -0.360967  2.331708 -0.374353    0.006538 -0.151506   \n",
       "4   -0.195088        -0.445380 -0.428870 -0.374353   -0.458789 -0.151506   \n",
       "\n",
       "      nb_qm    nb_and  nb_or     nb_eq  ...  empty_title  domain_in_title  \\\n",
       "0 -0.384054 -0.211981    0.0 -0.307427  ...     -0.37083         0.545898   \n",
       "1 -0.384054 -0.211981    0.0 -0.307427  ...     -0.37083         0.545898   \n",
       "2 -0.384054 -0.211981    0.0 -0.307427  ...     -0.37083        -1.831846   \n",
       "3  2.391141  2.310396    0.0  2.843696  ...     -0.37083         0.545898   \n",
       "4 -0.384054 -0.211981    0.0 -0.307427  ...     -0.37083        -1.831846   \n",
       "\n",
       "   domain_with_copyright  whois_registered_domain  domain_registration_length  \\\n",
       "0              -0.890858                 -0.27377                   -0.200046   \n",
       "1               1.122513                 -0.27377                   -0.538596   \n",
       "2               1.122513                 -0.27377                    0.582413   \n",
       "3               1.122513                 -0.27377                   -0.336309   \n",
       "4               1.122513                 -0.27377                    0.582413   \n",
       "\n",
       "   domain_age  web_traffic  dns_record  google_index  page_rank  \n",
       "0   -1.303867    -0.427241   -0.135494      0.917005  -1.255602  \n",
       "1    0.650155    -0.426863   -0.135494      0.917005   1.119788  \n",
       "2    0.980268    -0.427235   -0.135494     -1.090507   1.515686  \n",
       "3   -0.686351    -0.427241   -0.135494      0.917005  -1.255602  \n",
       "4    0.980268    -0.427235   -0.135494     -1.090507   1.515686  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(data=X, columns=df.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97846698",
   "metadata": {},
   "source": [
    "Now that our data is scaled, we want to split our dataset into a training and testing set. Our testing set will allow us to test our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d408b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (2640, 87) (2640,)\n",
      "Test set: (1132, 87) (1132,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# encode the training sets\n",
    "y_train = (y_train == 'legitimate').astype(int)\n",
    "y_test = (y_test == 'legitimate').astype(int)\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff3a80",
   "metadata": {},
   "source": [
    "# **5. Modeling** <a class=\"anchor\" id=\"5\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Our data is ready to train our model. In this notebook, we will be using tenserflow to train our model. Neural Network models consists of an input layer, hidden layer, and output layer. We will define each layer, and add them to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e37282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "fnn_model = Sequential()\n",
    "\n",
    "# input layer neruons count equal to number of features and ReLU activation\n",
    "input_layer = Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu')\n",
    "\n",
    "# hidden layer with 64 neurons for the first, and 32 for the second, and ReLU activation\n",
    "hidden_layer_one = Dense(64, activation = 'relu')\n",
    "hidden_layer_two = Dense(32, activation = 'relu')\n",
    "\n",
    "# output layer with 1 neuron and sigmoid activation (binary classification)\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "# add the layers to the model\n",
    "fnn_model.add(input_layer)\n",
    "fnn_model.add(hidden_layer_one)\n",
    "fnn_model.add(hidden_layer_two)\n",
    "fnn_model.add(output_layer)\n",
    "\n",
    "# compile the model\n",
    "fnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64bdab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8521 - loss: 0.4343 - val_accuracy: 0.9284 - val_loss: 0.1779\n",
      "Epoch 2/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - accuracy: 0.9432 - loss: 0.1573 - val_accuracy: 0.9382 - val_loss: 0.1549\n",
      "Epoch 3/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9670 - loss: 0.1064 - val_accuracy: 0.9382 - val_loss: 0.1648\n",
      "Epoch 4/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.9646 - loss: 0.1044 - val_accuracy: 0.9435 - val_loss: 0.1453\n",
      "Epoch 5/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - accuracy: 0.9669 - loss: 0.0852 - val_accuracy: 0.9452 - val_loss: 0.1447\n",
      "Epoch 6/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9771 - loss: 0.0702 - val_accuracy: 0.9435 - val_loss: 0.1532\n",
      "Epoch 7/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9778 - loss: 0.0645 - val_accuracy: 0.9337 - val_loss: 0.2008\n",
      "Epoch 8/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9853 - loss: 0.0514 - val_accuracy: 0.9443 - val_loss: 0.1570\n",
      "Epoch 9/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9897 - loss: 0.0373 - val_accuracy: 0.9373 - val_loss: 0.1777\n",
      "Epoch 10/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9933 - loss: 0.0258 - val_accuracy: 0.9496 - val_loss: 0.1694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2cd646eaac0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "fnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f700f18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "y_pred = fnn_model.predict(X_test)\n",
    "y_pred = (y_pred >= 0.5).astype(int) # round probabilities to get binary labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c429135",
   "metadata": {},
   "source": [
    "# **6. Evaluation** <a class=\"anchor\" id=\"6\"></a>\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5117d2fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427us/step\n",
      "Model accuracy score: 0.9496\n",
      "Training-set accuracy score: 0.9932\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = fnn_model.predict(X_train)\n",
    "y_pred_train = (y_pred_train >= 0.5).astype(int) # round probabilities to get binary labels\n",
    "    \n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e69157b",
   "metadata": {},
   "source": [
    "The trainin-set accuracy score is 0.9440 while the test-set accuracy is 0.9947. The two values differ by about 5%, which indicates there is little overfitting in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8d49108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null accuracy score: 0.5044\n"
     ]
    }
   ],
   "source": [
    "# return the most frequent value\n",
    "most_frequent = (y_test == 0).sum() if (y_test == 0).sum() >= (y_test == 1).sum() else (y_test == 1).sum()\n",
    "\n",
    "null_accuracy = most_frequent/len(y_test)\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28daa9",
   "metadata": {},
   "source": [
    "Our model accuracy score is 0.9470 while our null accuracy is 0.5044. Having a model accuracy score that is higher than the null accuracy is indicative that our model is doing a good job at predicting the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d3414",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
    "\n",
    "\n",
    "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
    "\n",
    "\n",
    "**True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
    "\n",
    "\n",
    "**True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
    "\n",
    "\n",
    "**False Positives (FP)** – False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n",
    "\n",
    "\n",
    "\n",
    "**False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n",
    "\n",
    "\n",
    "\n",
    "These four outcomes are summarized in a confusion matrix given below.\n",
    "\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c364014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "\n",
      " [[536  25]\n",
      " [ 32 539]]\n",
      "\n",
      "True Positives(TP) =  536\n",
      "\n",
      "True Negatives(TN) =  539\n",
      "\n",
      "False Positives(FP) =  25\n",
      "\n",
      "False Negatives(FN) =  32\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1964277e",
   "metadata": {},
   "source": [
    "The confusion matrix shows `545 + 527 = 1072 correct predictions` and `16 + 44 = 60 incorrect predictions`.\n",
    "\n",
    "\n",
    "In this case, we have\n",
    "\n",
    "\n",
    "- `True Positives` (Actual Positive:1 and Predict Positive:1) - 545\n",
    "\n",
    "\n",
    "- `True Negatives` (Actual Negative:0 and Predict Negative:0) - 527\n",
    "\n",
    "\n",
    "- `False Positives` (Actual Negative:0 but Predict Positive:1) - 16 `(Type I error)`\n",
    "\n",
    "\n",
    "- `False Negatives` (Actual Positive:1 but Predict Negative:0) - 44 `(Type II error)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7aca9c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAGdCAYAAAB3v4sOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDo0lEQVR4nO3deXxM9/4/8NeRZbKPJLKLBAlaglhLL7EEVeulDQ1K8auiCGIJJbYmVy6iG9UWQYvetvSiqmIXqiLELrVEbJnGksUSWSbn94evczuZ0DlxJpNJX8/7OI9H5nM+85n3zB3NO+/P53yOIIqiCCIiIqI/qWbqAIiIiKjyYYJAREREepggEBERkR4mCERERKSHCQIRERHpYYJAREREepggEBERkR4mCERERKSHCQIRERHpsTR1AE/Z+Q0ydQhElc6jjGhTh0BUSdUz6ui2td5SbKz8axsUG6siVZoEgYiIqLIQBBbY+QkQERGRHlYQiIiIShH49zMTBCIiotI4xcAEgYiISA8TBK5BICIiojKwgkBERFSKIAimDsHkmCAQERHpYYGdnwARERHpYQWBiIioFC5SZIJARESkhwkCpxiIiIioDKwgEBERlcKdFJkgEBER6eEUA6cYiIiIqAysIBAREZXCCgITBCIiIj1MEJggEBER6RHArZaZIhEREZEeVhCIiIhK4RQDEwQiIiI9TBA4xUBERERlYAWBiIioFFYQWEEgIiIqQzUFD8PNmTMHgiDoHJ6entJ5URQxZ84ceHt7w9bWFh06dMDZs2d1xigoKMC4ceNQo0YN2Nvbo3fv3rhx40a5PgEiIiKqJBo2bIjMzEzpOH36tHQuLi4OS5Yswaeffork5GR4enqiS5cuuH//vtQnIiICmzdvxsaNG5GUlIQHDx6gZ8+e0Gq1suLgFAMREVEpppxisLS01KkaPCWKIpYuXYqZM2eiX79+AIA1a9bAw8MD69evx6hRo5Cbm4uVK1di3bp1CA0NBQB8/fXX8PX1xa5du9CtWzeD42AFgYiIqBRBqKbYUVBQgLy8PJ2joKDgma998eJFeHt7o3bt2hg4cCCuXLkCAEhPT4dGo0HXrl2lviqVCiEhITh8+DAAICUlBUVFRTp9vL290ahRI6mPoZggEBERGVFsbCzUarXOERsbW2bf1q1bY+3atfjll1/w5ZdfQqPRoG3btrh79y40Gg0AwMPDQ+c5Hh4e0jmNRgNra2s4Ozs/s4+hOMVARERUiqDg389RUVGYNGmSTptKpSqzb/fu3aWfg4KC0KZNG9StWxdr1qzBK6+88iQ2QXcbaFEU9dpKM6RPaawgEBERlaLkFINKpYKTk5PO8awEoTR7e3sEBQXh4sWL0rqE0pWArKwsqarg6emJwsJCZGdnP7OPoZggEBERlVL6UsMXOV5EQUEBzp8/Dy8vL9SuXRuenp5ITEyUzhcWFmL//v1o27YtAKB58+awsrLS6ZOZmYkzZ85IfQzFKQYiIqJKIjIyEr169UKtWrWQlZWFBQsWIC8vD0OHDoUgCIiIiEBMTAwCAwMRGBiImJgY2NnZITw8HACgVqsxYsQITJ48Ga6urnBxcUFkZCSCgoKkqxoMxQSBiIioFFNd5njjxg289dZbuHPnDtzc3PDKK6/gyJEj8PPzAwBMnToV+fn5GDNmDLKzs9G6dWvs3LkTjo6O0hjx8fGwtLREWFgY8vPz0blzZyQkJMDCwkJWLIIoiqKi766c7PwGmToEokrnUUa0qUMgqqTqGXV0vyYxio2VcXKGYmNVJK5BICIiIj2cYiAiIiqFN2tigkBERKSHCQKnGIiIiKgMsisI//znP8u8rlMQBNjY2CAgIADh4eGoX7++IgESERFVNCV3UjRXsj8BtVqNPXv24Pjx41KicOLECezZswfFxcX49ttv0aRJExw6dEjxYImIiCqEUE25w0zJriB4enoiPDwcn376KapVe/LGS0pKMGHCBDg6OmLjxo147733MG3aNCQlJSkeMBERERmf7H0Q3NzccOjQIdSrp3sN6u+//462bdvizp07OH36NNq1a4ecnByDx+U+CET6uA8C0bMYdx+Eus2XKjbW5ZQIxcaqSLJrH8XFxbhw4YJe+4ULF6DVagEANjY2L7z/NBERkalUlnsxmJLsKYYhQ4ZgxIgRmDFjBlq2bAlBEHD06FHExMTg7bffBgDs378fDRs2VDxYIiKiisBFiuVIEOLj4+Hh4YG4uDj88ccfAAAPDw9MnDgR06ZNAwB07doVr732mrKREhERUYV5oXsx5OXlAQCcnJxeOBCuQSDSxzUIRM9i3DUI9Vp+pthYvyePVWysivRCOykqkRgQERFVOma8dkApsidZ/vjjDwwZMgTe3t6wtLSEhYWFzkFERETmT3YFYdiwYbh27RpmzZoFLy8vs16hSUREVCauUZSfICQlJeHgwYNo2rSpEcIhIiKqBPjHr/wcydfXFy+wrpGIiIjMgOwEYenSpZg+fTquXr1qhHCIiIgqAUFQ7jBTsqcYBgwYgEePHqFu3bqws7ODlZWVzvl79+4pFhwREZFJcA2C/ARh6dKlRgiDiIiIKhPZCcLQoUONEQcREVGlIZrx1IBSDEoQ8vLypE2Rnu6e+CzcPImIiMwe8wPDEgRnZ2dkZmbC3d0d1atXL3PvA1EUIQiCdEdHIiIis1WNGYJBCcKePXvg4uICANi7d69RAyIiIiLTMyhBCAkJKfNnIiKiKolrEMp3s6acnBwcPXoUWVlZKCkp0Tn39ttvKxIYERGRyTA/kJ8gbN26FYMGDcLDhw/h6Oiosx5BEAQmCERERFWA7K0gJk+ejOHDh+P+/fvIyclBdna2dHCTJCIiqhKqCcodZkp2BeHmzZsYP3487OzsjBEPERGR6XENgvwKQrdu3XDs2DFjxEJERESVhEEVhC1btkg/9+jRA1OmTMG5c+cQFBSkdy+G3r17KxshERFRRWMBwbAEoW/fvnpt8+bN02vjRklERFQlmPHaAaUYlCCUvpSRiIiIqjZFbmiZk5OjxDBERESVg6DgYaZkJwgLFy7Et99+Kz1+88034eLiAh8fH5w8eVLR4IiIiExBFATFDnMlO0FYsWIFfH19AQCJiYnYtWsXduzYge7du2PKlCmKB0hERFThuA+C/H0QMjMzpQRh27ZtCAsLQ9euXeHv74/WrVsrHiARERFVPNkVBGdnZ1y/fh0AsGPHDoSGhgJ4crtnXsFARERVAtcgyK8g9OvXD+Hh4QgMDMTdu3fRvXt3AEBqaioCAgIUD5CIiKjCmfHaAaXIThDi4+Ph7++P69evIy4uDg4ODgCeTD2MGTNG8QCJiIio4slOEKysrBAZGanXHhERoUQ8REREpmfGiwuVYvBWy927d4eVlZXOtstl4VbLRERk9pgfGL7Vskajgbu7e5nbLj/FrZaJiIiqBtlbLXPbZSIiqvK4SFH+GgQiIqIqjwlC+RKE3bt3Y/fu3cjKytKrKKxatUqRwIiIiMh0ZCcIc+fOxbx589CiRQt4eXlBYJZFRERVjSK3MjRvshOEzz//HAkJCRgyZIgx4iEiIjI9/vErP0EoLCxE27ZtjRELERFR5cD8QH4RZeTIkVi/fr0xYiEiIqJKwqAKwqRJk6SfS0pK8MUXX2DXrl1o3LgxrKysdPouWbJE2QiJiIgqmMidFA1LEE6cOKHzuGnTpgCAM2fO6LRzwWLlNDOiH2ZO7K/T9kdWDmq3HCudf6NXG9T0dkFhkRYnTqdj7r//g+TUyzrPadUsAHOmhKFl07ooKtLi1LkM9B0ah8cFRRX2XoiMZcWK77Bz52FcuXITNjbWCA5ugMjIYahTp6bUZ/r0eGzevEfneU2a1Md//rOoosMlY+PvM8MShL179xo7DjKys2nX0XNQrPRYq/3f5akX0zWYNDsB6deyYGtjjXEju2PLuukICpmEO/fuA3iSHPx3zTQsWrYFk2evQWFRMYJe9kOJKFb4eyEyhqNHz2DQoB4ICgqEVluC+Pi1GDFiNn76aRns7Gykfu3aNUNsbIT02MqK28lQ1STrm52RkYGdO3eiuLgYISEhePnll40VFylMW1yCP27nlnnuP/89rPN42vxvMGxgRzR6qRb2HToLAIibNQTLE37B4uVbpX6Xr/5hvICJKtjKlXN1HsfGRqBNm8E4e/YSWrZsJLVbW1vBzc25osOjisYCguEJwoEDB/D666/j0aNHT55oaYk1a9bgrbfeMlpwpJy6tT1w+einKCgsQvKJy4iO+xZXr9/W62dlZYHh4R2Rk/sQp89lAADcXJ3QqlkANv73EPZsikbtWh74/fItzPn3f/Drsd8r+q0QVYj79x8CANRqR532o0fPoE2bwXByskfLlo0wceIQuLpWN0GEZFRcgwBBFA2rEYeEhMDJyQkrVqyAra0toqKi8NNPP+H69euKBGLnN0iRcUhf1w5NYGtrjUtXNHCv4YRp4/qifl1vNO8yDfdyHgAAuncKxppP34edrTU0WTkY8P/ikXLqCgCgZXAA9v84F3ez72PGh+tx6lwGwvu1w7tDQtGi6zRWEozoUUa0qUP4WxJFEaNHL0Be3gOsX79Qat++/SDs7Gzg7e2OGzf+wEcffQ2tVotNm5bC2trqOSOS8uoZdfS6QzYqNtbldQMVG6siGZwguLi44MCBA2jU6Emp7eHDh3BycsKdO3fg7Cyv3FZQUICCggKdNo9G70IQLGSNQ+VjZ6vC2QNLsGTFNnzy1c9Sm6d7dbi6OGL4Wx0R0vZlhPSJxu27eWjdPBB7N83Bvz/9L6L//R9pnN92xGLHnlREx31rqrdS5TFBMI25c5dj//5jWL9+ITw9azyzX1bWPXTqNAJLlkxB167cH6ZiGTlBeFu5/65dXjtAsbEqksH7IOTk5MDd3V16bG9vDzs7O+Tk5Mh+0djYWKjVap2jOPes7HGofB7lF+BM2nUE+HvqtF3J+APJJy5h9NQvUVxcgqEDOgAANFk5AIDzl27qjJN26RZ8fVwrKmyiCjF//grs2XMUa9Z8+NzkAADc3V3g7e2Gq1dvVVB0VGEEBQ8zJWuR4rlz56DRaKTHoiji/PnzuH//vtTWuHHjvxwnKipKZ28F4EkFgSqGtbUlGgT44PDRtGf2EQRAZf3k65Fx/TZuae6hXh0vnT6BdTyxc+9Jo8ZKVFFEUcT8+SuQmPgr1q2Lha+v518+Jzs7D5mZd+Du7lIBERJVLFkJQufOnVF6RqJnz54QBAGiKEIQBGi12r8cR6VSQaVS6bRxesF4YmaGY/uu47h+6y7cXZ+sQXB0sMXXPxyEna0K097vg227jkOTlQNXZwe8OyQUPp4u2PTTb9IY8St+wgcT++PU+Ws4dTYDg99oh3p1vRH+3kcmfGdEypk7dzm2bTuAZctmwt7eFrdvZwMAHB3tYGOjwsOH+fj00/Xo2vVVuLk54+bNLMTHr4WzsxNCQ18xcfSkOC5SNDxBSE9PN2YcZEQ+ni5Y88n7cHV2xJ17eTh64hI6/DMa12/egUplhXoB3tjwRju4OjviXs4DpJy8gi5vzsf5i/+bUvhs1Q7YqKwQN2swnKvb4/T5a+g5KBbp17JM+M6IlLNhw5P1OEOGzNBpj42dgH79QmFhUQ2//56BH3/ci/v3H8LNzRmtWwchPn4qHBzsTBEyGVMlSBBiY2MxY8YMTJgwAUuXLgXwpNI1d+5cfPHFF8jOzkbr1q3x2WefoWHDhtLzCgoKEBkZiQ0bNiA/Px+dO3fGsmXLULNmzWe8UtkMXqRobLyKgUgfFykSPYtxFynWGfmdYmNd+epN2c9JTk5GWFgYnJyc0LFjRylBWLhwIT788EMkJCSgXr16WLBgAQ4cOIC0tDQ4Oj65JHf06NHYunUrEhIS4OrqismTJ+PevXtISUmBhYXh1Xre8ZqIiKgSefDgAQYNGoQvv/xS5ypBURSxdOlSzJw5E/369UOjRo2wZs0aPHr0SLqJYm5uLlauXInFixcjNDQUwcHB+Prrr3H69Gns2rVLVhxMEIiIiEqrJih3yDR27Fj06NEDoaGhOu3p6enQaDTo2rWr1KZSqRASEoLDh5/siJuSkoKioiKdPt7e3mjUqJHUx1DcRJyIiKg0BW/WVNbeP2Ut1geAjRs3IiUlBceOHdM79/QqQg8PD512Dw8PZGRkSH2sra319ify8PDQuQrREKwgEBERGVFZe//Exsbq9bt+/TomTJiAb775BjY2NmWM9ETpOyc/vYrweQzpU5rsBKFTp05lbo6Ul5eHTp06yR2OiIio8lFwiiEqKgq5ubk6R1RUlN5LpqSkICsrC82bN4elpSUsLS2xf/9+fPzxx7C0tJQqB6UrAVlZWdI5T09PFBYWIjs7+5l9DP4IZPUGsG/fPhQWFuq1P378GAcPHpQ7HBERUeVTTblDpVLByclJ5yhreqFz5844ffo0UlNTpaNFixYYNGgQUlNTUadOHXh6eiIxMVF6TmFhIfbv34+2bZ9s9d28eXNYWVnp9MnMzMSZM2ekPoYyeA3CqVOnpJ9L76io1WqxY8cO+Pj4yHpxIiIiesLR0VG639FT9vb2cHV1ldojIiIQExODwMBABAYGIiYmBnZ2dggPDwcAqNVqjBgxApMnT4arqytcXFwQGRmJoKAgvUWPf8XgBKFp06YQBAGCIJQ5lWBra4tPPvlE1osTERFVSgouUlTS1KlTkZ+fjzFjxkgbJe3cuVPaAwEA4uPjYWlpibCwMGmjpISEBFl7IAAyNkrKyMiAKIqoU6cOjh49Cjc3N+mctbU13N3dZb/4n3GjJCJ93CiJ6FmMvFHS+B8VG+vKx30VG6siGVxB8PPzAwCUlJQYLRgiIiKqHGQvUoyNjcWqVav02letWoWFCxcqEhQREZEpiYKg2GGuZCcIK1asQIMGDfTaGzZsiM8//1yRoIiIiExKwasYzJXsnRQ1Gg28vLz02t3c3JCZmalIUERERCZVCe7maGqycxtfX18cOnRIr/3QoUPw9vZWJCgiIiIyLdkVhJEjRyIiIgJFRUXS5Y67d+/G1KlTMXnyZMUDJCIiqnBmvHZAKbIThKlTp+LevXsYM2aMtKOijY0Npk2bVubWkURERGaHUwzyEwRBELBw4ULMmjUL58+fh62tLQIDA8vcNpKIiIjMU7lv9+zg4ICWLVsqGQsREVHlwAKCYQlCv379kJCQACcnJ/Tr1++5fTdt2qRIYERERKYicorBsARBrVZL95FWq9VGDYiIiIhMz6AEYfXq1WX+TEREVCWxglD+NQhERERVFi9zNCxBCA4OlqYY/srx48dfKCAiIiIyPYMShL59+0o/P378GMuWLcPLL7+MNm3aAACOHDmCs2fPYsyYMUYJkoiIqEKZ8T0UlGJQghAd/b970o8cORLjx4/H/Pnz9fpcv35d2eiIiIhMgVMM8nOk7777Dm+//bZe++DBg/HDDz8oEhQREZFJVROUO8yU7ATB1tYWSUlJeu1JSUmwsbFRJCgiIiIyLdlXMURERGD06NFISUnBK6+8AuDJGoRVq1Zh9uzZigdIRERU4cz4L3+lyE4Qpk+fjjp16uCjjz7C+vXrAQAvvfQSEhISEBYWpniAREREFU3kGoTy7YMQFhbGZICIiKgKK9eFHDk5Ofjqq68wY8YM3Lt3D8CT/Q9u3rypaHBEREQmUU3Bw0zJriCcOnUKoaGhUKvVuHr1KkaOHAkXFxds3rwZGRkZWLt2rTHiJCIiqjicYpCf20yaNAnDhg3DxYsXda5a6N69Ow4cOKBocERERGQasisIycnJWLFihV67j48PNBqNIkERERGZFK9ikJ8g2NjYIC8vT689LS0Nbm5uigRFRERkUkwQ5E8x9OnTB/PmzUNRUREAQBAEXLt2DdOnT0f//v0VD5CIiIgqnuwEYdGiRbh9+zbc3d2Rn5+PkJAQBAQEwNHRER9++KExYiQiIqpYgoKHmZI9xeDk5ISkpCTs2bMHx48fR0lJCZo1a4bQ0FBjxEdERFThRE4xyEsQiouLYWNjg9TUVHTq1AmdOnUyVlxERESmw8sc5U0xWFpaws/PD1qt1ljxEBERUSUgew3CBx98gKioKGkHRSIioiqHt3uWvwbh448/xqVLl+Dt7Q0/Pz/Y29vrnD9+/LhiwREREZmE+f5eV4zsBKFPnz4QODdDRERUpclOEObMmWOEMIiIiCqPamZ8kyWlGPwRPHr0CGPHjoWPjw/c3d0RHh6OO3fuGDM2IiIikxAE5Q5zZXCCEB0djYSEBPTo0QMDBw5EYmIiRo8ebczYiIiIyEQMnmLYtGkTVq5ciYEDBwIABg8ejFdffRVarRYWFhZGC5CIiKiimfNf/koxuIJw/fp1tGvXTnrcqlUrWFpa4tatW0YJjIiIyFQEQVDsMFcGVxC0Wi2sra11n2xpieLiYsWDIiIiMiUz/r2uGIMTBFEUMWzYMKhUKqnt8ePHeO+993T2Qti0aZOyERIREVGFMzhBGDp0qF7b4MGDFQ2GiIioMmAFQUaCsHr1amPGQUREVGkI3AdB/r0YiIiIqOqTvZMiERFRVccpBiYIREREesz4JoyK4RQDERER6WEFgYiIqBROMTBBICIi0sMEgVMMREREVAZWEIiIiEox53soKIUJAhERUSncKIkJAhERkR4WELgGgYiIiMrACgIREVEprCAwQSAiItLDBIFTDERERFQGVhCIiIhK4b0YmCAQERHp4RQDpxiIiIioDKwgEBERlcIKAisIREREeoRqgmKHHMuXL0fjxo3h5OQEJycntGnTBj///LN0XhRFzJkzB97e3rC1tUWHDh1w9uxZnTEKCgowbtw41KhRA/b29ujduzdu3Lgh+zNggkBERFRJ1KxZE//6179w7NgxHDt2DJ06dUKfPn2kJCAuLg5LlizBp59+iuTkZHh6eqJLly64f/++NEZERAQ2b96MjRs3IikpCQ8ePEDPnj2h1WplxSKIoigq+u7Kyc5vkKlDIKp0HmVEmzoEokqqnlFHb/VdkmJjHX3zHy/0fBcXF/z73//G8OHD4e3tjYiICEybNg3Ak2qBh4cHFi5ciFGjRiE3Nxdubm5Yt24dBgwYAAC4desWfH19sX37dnTr1s3g12UFgYiIqBRBUO4oL61Wi40bN+Lhw4do06YN0tPTodFo0LVrV6mPSqVCSEgIDh8+DABISUlBUVGRTh9vb280atRI6mMoLlIkIiIqRclFigUFBSgoKNBpU6lUUKlUZfY/ffo02rRpg8ePH8PBwQGbN2/Gyy+/LP2C9/Dw0Onv4eGBjIwMAIBGo4G1tTWcnZ31+mg0Gllxs4JARERkRLGxsVCr1TpHbGzsM/vXr18fqampOHLkCEaPHo2hQ4fi3Llz0nmhVPYiiqJeW2mG9CmNFQQiIqJSlNxJMSoqCpMmTdJpe1b1AACsra0REBAAAGjRogWSk5Px0UcfSesONBoNvLy8pP5ZWVlSVcHT0xOFhYXIzs7WqSJkZWWhbdu2suJmBYGIiKgUJdcgqFQq6bLFp8fzEoTSRFFEQUEBateuDU9PTyQmJkrnCgsLsX//fumXf/PmzWFlZaXTJzMzE2fOnJGdILCCQEREVEnMmDED3bt3h6+vL+7fv4+NGzdi37592LFjBwRBQEREBGJiYhAYGIjAwEDExMTAzs4O4eHhAAC1Wo0RI0Zg8uTJcHV1hYuLCyIjIxEUFITQ0FBZsTBBICIiKkUwUX39jz/+wJAhQ5CZmQm1Wo3GjRtjx44d6NKlCwBg6tSpyM/Px5gxY5CdnY3WrVtj586dcHR0lMaIj4+HpaUlwsLCkJ+fj86dOyMhIQEWFhayYuE+CESVGPdBIHoW4+6D0G6LcvsgHOz9YvsgmArXIBAREZEeTjEQERGVIveSwKqICQIREVEpzA84xUBERERlYAWBiIioFFYQmCAQERHpYYJQiRKEB1dnmjoEokrHthYvcyQqS/61DUYdX8mtls0V1yAQERGRnkpTQSAiIqosWEFggkBERKSnmlApNhk2KU4xEBERkR5WEIiIiErhFAMTBCIiIj0sr/MzICIiojKwgkBERFQKFykyQSAiItLDNQicYiAiIqIysIJARERUCv96ZoJARESkh1MMTBCIiIj0CFykyCoKERER6WMFgYiIqBROMTBBICIi0sPyOj8DIiIiKgMrCERERKVwJ0UmCERERHq4BoFTDERERFQGVhCIiIhK4V/PTBCIiIj0cIqBSRIRERGVgRUEIiKiUngVAxMEIiIiPZxiYIJARESkh/Pv/AyIiIioDKwgEBERlcI1CEwQiIiI9HANAqcYiIiIqAwvVEF4/PgxbGxslIqFiIioUmAFoRwVhJKSEsyfPx8+Pj5wcHDAlStXAACzZs3CypUrFQ+QiIioolVT8DBXsmNfsGABEhISEBcXB2tra6k9KCgIX331laLBERERkWnIThDWrl2LL774AoMGDYKFhYXU3rhxY1y4cEHR4IiIiEyhmiAqdpgr2WsQbt68iYCAAL32kpISFBUVKRIUERGRKXENQjkqCA0bNsTBgwf12r/77jsEBwcrEhQRERGZluwKQnR0NIYMGYKbN2+ipKQEmzZtQlpaGtauXYtt27YZI0YiIqIKZc6LC5Ui+zPo1asXvv32W2zfvh2CIGD27Nk4f/48tm7dii5duhgjRiIiogpVTVDuMFfl2gehW7du6Natm9KxEBERVQqCGS8uVIrsCkKdOnVw9+5dvfacnBzUqVNHkaCIiIjItGRXEK5evQqtVqvXXlBQgJs3byoSFBERkSmZ89SAUgxOELZs2SL9/Msvv0CtVkuPtVotdu/eDX9/f0WDIyIiMgUuUpSRIPTt2xcAIAgChg4dqnPOysoK/v7+WLx4saLBERERkWkYnCCUlJQAAGrXro3k5GTUqFHDaEERERGZkjnvgKgU2WsQ0tPTjREHERFRpcE1COW8zPHhw4fYv38/rl27hsLCQp1z48ePVyQwIiIiMh3ZCcKJEyfw+uuv49GjR3j48CFcXFxw584d2NnZwd3dnQkCERGZPVYQyrFQc+LEiejVqxfu3bsHW1tbHDlyBBkZGWjevDkWLVpkjBiJiIgqlIWCh7mSnSCkpqZi8uTJsLCwgIWFBQoKCuDr64u4uDjMmDHDGDESERFRBZOdIFhZWUEQntRePDw8cO3aNQCAWq2WfiYiIjJn1QRRscNcyV6DEBwcjGPHjqFevXro2LEjZs+ejTt37mDdunUICgoyRoxEREQVimsQylFBiImJgZeXFwBg/vz5cHV1xejRo5GVlYUvvvhC8QCJiIgqGu/mWI4KQosWLaSf3dzcsH37dkUDIiIiItMr1z4IREREVZmFGf/lrxTZUwx3797F2LFj8fLLL6NGjRpwcXHROYiIiMydqaYYYmNj0bJlSzg6OsLd3R19+/ZFWlqaTh9RFDFnzhx4e3vD1tYWHTp0wNmzZ3X6FBQUYNy4cahRowbs7e3Ru3dv3LhxQ1YssisIgwcPxuXLlzFixAh4eHhIVzQQERHRi9m/fz/Gjh2Lli1bori4GDNnzkTXrl1x7tw52NvbAwDi4uKwZMkSJCQkoF69eliwYAG6dOmCtLQ0ODo6AgAiIiKwdetWbNy4Ea6urpg8eTJ69uyJlJQUWFgYtjuDIIqirGswHB0dkZSUhCZNmsh8289XIp5TdDyiqsDeb76pQyCqlPKvbTDq+B+d3anYWBMadi33c2/fvg13d3fs378f7du3hyiK8Pb2RkREBKZNmwbgSbXAw8MDCxcuxKhRo5Cbmws3NzesW7cOAwYMAADcunULvr6+2L59O7p162bQa8ueYmjQoAHy8/PlPo2IiMhsKDnFUFBQgLy8PJ2joKDAoDhyc3MBQJrCT09Ph0ajQdeu/0s6VCoVQkJCcPjwYQBASkoKioqKdPp4e3ujUaNGUh+DPgODe/6fZcuWYebMmdi/fz/u3r2r96aJiIjof2JjY6FWq3WO2NjYv3yeKIqYNGkS/vGPf6BRo0YAAI1GA+DJRoV/5uHhIZ3TaDSwtraGs7PzM/sYQvYahOrVqyM3NxedOnXSeyOCIECr1codkoiIqFJR8h4KUVFRmDRpkk6bSqX6y+e9//77OHXqFJKSkvTOlV7/9/R38PMY0ufPZCcIgwYNgrW1NdavX89FikREVCUpucGRSqUyKCH4s3HjxmHLli04cOAAatasKbV7enoCeFIleLppIQBkZWVJVQVPT08UFhYiOztbp4qQlZWFtm3bGhyD7AThzJkzOHHiBOrXry/3qURERPQcoihi3Lhx2Lx5M/bt24fatWvrnK9duzY8PT2RmJiI4OBgAEBhYSH279+PhQsXAgCaN28OKysrJCYmIiwsDACQmZmJM2fOIC4uzuBYyrWT4vXr15kgEBFRlWWqmyyNHTsW69evx3//+184OjpKawbUajVsbW0hCAIiIiIQExODwMBABAYGIiYmBnZ2dggPD5f6jhgxApMnT4arqytcXFwQGRmJoKAghIaGGhyL7ARh3LhxmDBhAqZMmYKgoCBYWVnpnG/cuLHcIYmIiCoVU+2kuHz5cgBAhw4ddNpXr16NYcOGAQCmTp2K/Px8jBkzBtnZ2WjdujV27twp7YEAAPHx8bC0tERYWBjy8/PRuXNnJCQkGLwHAlCOfRCqVdO/8EEQhBdepMh9EIj0cR8EorIZex+E1b//othY79QzbN+BykZ2BSE9Pd0YcRAREVElIjtB8PPzM0YcRERElYY536ZZKQYlCFu2bEH37t1hZWWFLVu2PLdv7969FQmMiIjIVJggGJgg9O3bFxqNRrqz1LNwoyQiIqKqwaAEoaSkpMyfiYiIqiILE13mWJnIvhfD2rVry7zJRGFhIdauXatIUERERKZUTcHDXMmO/Z133pHuLvVn9+/fxzvvvKNIUERERGRasq9ieNbNHm7cuAG1Wq1IUERERKbERYoyEoTg4GAIggBBENC5c2dYWv7vqVqtFunp6XjttdeMEiQREVFFYoIgI0F4evVCamoqunXrBgcHB+mctbU1/P390b9/f8UDJCIioopncIIQHR0NAPD398eAAQNgY2NjtKCIiIhMiVcxlGMNwtChQ40RBxERUaXBKQYDEwQXFxf8/vvvqFGjBpydnctcpPjUvXv3FAuOiIjIFJggGJggxMfHS7eRjI+Pf26CQERERObPoAThz9MKT+9HTUREVFWxglCONQh5eXlltguCAJVKBWtr6xcOioiIyJQsmCDITxCqV6/+3CmGmjVrYtiwYYiOjka1aua8ySQREdHfl+wEISEhATNnzsSwYcPQqlUriKKI5ORkrFmzBh988AFu376NRYsWQaVSYcaMGcaImYiIyKiq8TJH+QnCmjVrsHjxYoSFhUltvXv3RlBQEFasWIHdu3ejVq1a+PDDD5kgEBGRWWL9uxyfwa+//org4GC99uDgYPz6668AgH/84x+4du3ai0dHREREJiE7QahZsyZWrlyp175y5Ur4+voCAO7evQtnZ+cXj46IiMgEqgnKHeZK9hTDokWL8Oabb+Lnn39Gy5YtIQgCkpOTceHCBXz//fcAgOTkZAwYMEDxYEkZGzbswMYNO3DzZhYAICDAF2PGhqF9++YoKirGRx+tx4H9Kbhx4w84ONihTdsmmDxpCNw9XEwcOZFyZk7sjw8mvqHTpsnKQe0Wo6Xzb/Zqg5rerigsKsaJ0+mYE/ctklMvS/1r+7njXzMHo03L+lBZWyJx/ylMmp2ArDu5FfpeSHm8igEQRFGUvRLj6tWr+Pzzz/H7779DFEU0aNAAo0aNgr+/f7kDKRHPlfu5JM/ePcmoZlENtWp5AgD+++NerFr1X/ywaTE8PV0xYcK/8eabXdCgvj9y8x4gNnYVtMVafP/DIhNH/vdj7zff1CFUWTMn9sc/X2+NHuEfSm1abQnu3LsPABjQpy2y7uYh/VoWbG2sMW5Ed/Tr8QoatY/AnXv3YWerQvLOhTh9LgPzlzz54yg68k14eTijfZ/ZKMd/WkmG/GsbjDr+/sztio0V4vW6YmNVpHIlCMbABMG0Xmk9BJFThuKNN0L1zp0+fRFhb07F7j1fwNvbzQTR/X0xQTCemRP7o1fXFnile5RB/R0dbJF1bhW6v7UA+w6dRed2Qfjv2unwChqJ+w/yAQDV1fbIPP0VXg//EHuTzhgz/L89YycIBzU/KTZWO88eio1Vkcq1UPPgwYMYPHgw2rZti5s3bwIA1q1bh6SkJEWDI+PTarX46aeDePToMZo2rV9mn/v3H0EQBDg52VdwdETGFVDbE1eSl+F80kdY++k4+NdyL7OflZUFRoR3Qk7uQ5w+92QBtkplBVEUUVBYJPV7/LgQWm0J2rYs+98SmQ+uQShHgvDDDz+gW7dusLW1xfHjx1FQUAAAuH//PmJiYhQPkIzj97QMNG/2Fpo0DsPcOZ/jk0+nIyDAV69fQUEhlixeh54928HBwc4EkRIZR/KJSxg5cTl6DY7FmOlfwsOtOvZumguX6g5Sn+6dg3H7/GrkXFyLcSNfR89BMbib/WQK4ujxi3j4qAAfRoXD1sYadrYqxM4cBAuLavB0r26id0VKYYJQjimG4OBgTJw4EW+//TYcHR1x8uRJ1KlTB6mpqXjttdeg0Wj+coyCggIpsXjKyvoKVCpu01xRCguLkJl5B/fzHmLnzl/x/fe7sHbdAp0koaioGBMj/o1bmXewdu18JggmwCmGimNnq8LZg0sR//lWfPzVdqnN0706arg44p23OqFD24Zo32cWbt99suV853ZB+DhmBPx93VBSIuI/Ww6jQaAPkk9cRsQHq0z5dqo8Y08x/Jql3BRDG/e/yRRDWloa2rdvr9fu5OSEnJwcg8aIjY2FWq3WOf4V+6XcUOgFWFtbwc/PC42CAjBp8hDUb+CPdWu3SeeLiooxceIi3LiRhZUro5kcUJX3KL8AZ9Ouo25tT522Kxl/4OiJSxg99QsUa7UYOrCjdH73wdNo2C4CtYLfQ82m72JExDJ4e7gg43qWKd4CKaiagoe5kn2Zo5eXFy5duqR3xUJSUhLq1Klj0BhRUVGYNGmSTpuV9RW5oZCSRBGF/zeX+jQ5yMi4hTVr5sPZ2cnEwREZn7W1JRoEeOPQ0QvP7CMIAlTW+v/ZfDrtENK2IdxrOGFbYorR4qSK8ZxbDv1tyE4QRo0ahQkTJmDVqlUQBAG3bt3Cr7/+isjISMyePdugMVQqFVQqlU5bicjphYoSv+RrtGvfDF6eNfDwYT62bz+Io0fP4osvZ6G4WIuICXE4d+4Kln8+E1ptCW7fzgYAqNUOsLa2MnH0RMqInTkIP+06juu37sDd1QnTxv8Tjg62+Ob7A7CzVWHauL74KTEFmqwcuDg74N0hXeDj6YJNP/0mjTHkzRCkXbqJ2/fy0LpZPSya8zY++epnXLySacJ3RqQM2QnC1KlTkZubi44dO+Lx48do3749VCoVIiMj8f777xsjRlLYnbs5mDZ1KW7fzoajox3q1ffHF1/OwquvNsXNG1nYsycZAPDPvrpVnjVr5qNV60amCJlIcT5eLlj76Ti4Ojvizr08HD1+ESF9Z+PazTtQqaxQv643Br/RHq7OjriX8wDHTl5G6Btzcf73G9IY9ep6Yd60gXCp7oCMG7cR98mP0voFMm8sILzAPgiPHj3CuXPnUFJSgpdffhkqlQqZmZmoVatWuQLhPghE+rhIkahsxl6keOyOcosUW9Qwz0WKsisIT9nZ2aFFixbS45MnT6JZs2bQarWKBEZERESmU+4EgYiIqKoy56sPlMIEgYiIqBRBqBR3ITApJklERESkx+AKwqlTp557Pi0t7YWDISIiqgx4FYOMBKFp06YQBKHMW5g+bRe4swQREVUB/HUmI0FIT083ZhxERESVBvMDGQmCn5+fMeMgIiKiSoRXMRAREZVizrdpVgoTBCIiolKYH/AyRyIiIioDKwhERESl8CqGclQQOnXqhJycHL32vLw8dOrUSYmYiIiITEpQ8DBXshOEffv2obCwUK/98ePHOHjwoCJBERERkWmVayfFc+fOQaPRSI+1Wi127NgBHx8fZaMjIiIyAXP+y18psndSFAShzKkEW1tbfPLJJ4oGR0REZAq8zFHmToqiKKJOnTo4evQo3NzcpHPW1tZwd3eHhYWFUYIkIiKiiiV7J8WSkhKjBUNERFQZsIBQjkWKsbGxWLVqlV77qlWrsHDhQkWCIiIiMiVBEBU7zJXsBGHFihVo0KCBXnvDhg3x+eefKxIUERGRKfEyx3IkCBqNBl5eXnrtbm5uyMzMVCQoIiIiMi3ZCYKvry8OHTqk137o0CF4e3srEhQREZEpCYJyh7mSvdXyyJEjERERgaKiIulyx927d2Pq1KmYPHmy4gESERFVNN6oqBwJwtSpU3Hv3j2MGTNG2lHRxsYG06ZNQ1RUlOIBEhERUcUTRFEs1xLLBw8e4Pz587C1tUVgYCBUKtULBVIinnuh5xNVRfZ+800dAlGllH9tg1HHz3iwVbGx/Bx6KTZWRSr33RwdHBzQsmVLJWMhIiKqFMx46YBiDEoQ+vXrh4SEBDg5OaFfv37P7btp0yZFAiMiIiLTMShBUKvVEP5vKaZarTZqQERERKZmzlcfKKXcaxCUxjUIRPq4BoGobMZeg3DjoXJrEGram+caBF7JQUREVEkcOHAAvXr1gre3NwRBwI8//qhzXhRFzJkzB97e3rC1tUWHDh1w9uxZnT4FBQUYN24catSoAXt7e/Tu3Rs3btyQHYtBUwzBwcHSFMNfOX78uOwgiIiIKhNT3e754cOHaNKkCd555x30799f73xcXByWLFmChIQE1KtXDwsWLECXLl2QlpYGR0dHAEBERAS2bt2KjRs3wtXVFZMnT0bPnj2RkpIi667LBiUIffv2lX5+/Pgxli1bhpdffhlt2rQBABw5cgRnz57FmDFjDH5hIiKiyspUSxC6d++O7t27l3lOFEUsXboUM2fOlC4YWLNmDTw8PLB+/XqMGjUKubm5WLlyJdatW4fQ0FAAwNdffw1fX1/s2rUL3bp1MzgWgxKE6Oho6eeRI0di/PjxmD9/vl6f69evG/zCRERElZWSd2EsKChAQUGBTptKpZK9f1B6ejo0Gg26du2qM05ISAgOHz6MUaNGISUlBUVFRTp9vL290ahRIxw+fFhWgiB7DcJ3332Ht99+W6998ODB+OGHH+QOR0REVKXFxsZCrVbrHLGxsbLH0Wg0AAAPDw+ddg8PD+mcRqOBtbU1nJ2dn9nHULI3SrK1tUVSUhICAwN12pOSkmBjYyN3OCIiokpHySmGqKgoTJo0SaftRXYfLr0mUBTFv1wnaEif0mQnCBERERg9ejRSUlLwyiuvAHiyBmHVqlWYPXu23OGIiIgqHSX3QSjPdEJZPD09ATypEnh5eUntWVlZUlXB09MThYWFyM7O1qkiZGVloW3btrJeT/YUw/Tp07F27VqcOHEC48ePx/jx43HixAkkJCRg+vTpcocjIiIiA9SuXRuenp5ITEyU2goLC7F//37pl3/z5s1hZWWl0yczMxNnzpyRnSCU614MYWFhCAsLK89TiYiIKj1TXcXw4MEDXLp0SXqcnp6O1NRUuLi4oFatWoiIiEBMTAwCAwMRGBiImJgY2NnZITw8HMCT3Y5HjBiByZMnw9XVFS4uLoiMjERQUJB0VYOhypUg5OTk4Pvvv8eVK1cQGRkJFxcXHD9+HB4eHvDx8SnPkERERJWGqXYRPHbsGDp27Cg9frp2YejQoUhISMDUqVORn5+PMWPGIDs7G61bt8bOnTulPRAAID4+HpaWlggLC0N+fj46d+6MhIQEWXsgAOXYavnUqVMIDQ2FWq3G1atXkZaWhjp16mDWrFnIyMjA2rVrZQXwFLdaJtLHrZaJymbsrZbvPt6i2FiuNr0VG6siyU6SJk2ahGHDhuHixYs6Vy10794dBw4cUDQ4IiIiUxAE5Q5zJXuKITk5GStWrNBr9/HxkX2NJRERUeVkxr/ZFSK7gmBjY4O8vDy99rS0NLi5uSkSFBEREZmW7AShT58+mDdvHoqKigA82bDh2rVrmD59epk3liAiIjI3goL/M1eyE4RFixbh9u3bcHd3R35+PkJCQhAQEABHR0d8+OGHxoiRiIioQglCNcUOcyV7DYKTkxOSkpKwZ88eHD9+HCUlJWjWrJns6yuJiIgqL/P9y18pshKE4uJi2NjYIDU1FZ06dUKnTp2MFRcRERGZkKwEwdLSEn5+ftBqtcaKh4iIyOTMee2AUmRPjnzwwQeIiorCvXv3jBEPERFRJSAoeJgn2WsQPv74Y1y6dAne3t7w8/ODvb29zvnjx48rFhwRERGZhuwEoU+fPrLvKU1ERGROzPnqA6XIThDmzJljhDCIiIgqE/4hbHCK9OjRI4wdOxY+Pj5wd3dHeHg47ty5Y8zYiIiIyEQMThCio6ORkJCAHj16YODAgUhMTMTo0aONGRsREZFJcCdFGVMMmzZtwsqVKzFw4EAAwODBg/Hqq69Cq9XKvsc0ERFRZWbOv9iVYnAF4fr162jXrp30uFWrVrC0tMStW7eMEhgRERGZjsEVBK1WC2tra90nW1qiuLhY8aCIiIhMi1cxGJwgiKKIYcOGQaVSSW2PHz/Ge++9p7MXwqZNm5SNkIiIqILxcn4ZCcLQoUP12gYPHqxoMERERJUDEwSDE4TVq1cbMw4iIiKqRGRvlERERFTV8SoGJghERERl4CJFfgJERESkhxUEIiKiUjjFwASBiIhIDy9z5BQDERERlYEVBCIiIj2sIDBBICIiKkVggZ2fABEREeljBYGIiEgPpxiYIBAREZXCqxiYIBAREZWBCQLXIBAREZEeVhCIiIhK4VUMTBCIiIjKwCkGpkhERESkhxUEIiKiUnizJiYIREREeniZI6cYiIiIqAysIBAREenh389MEIiIiErhGgSmSERERFQGVhCIiIj0sILABIGIiKgUXsXABIGIiKgMnIHnJ0BERER6WEEgIiIqhVcxAIIoiqKpg6DKo6CgALGxsYiKioJKpTJ1OESVAv9d0N8REwTSkZeXB7VajdzcXDg5OZk6HKJKgf8u6O+IaxCIiIhIDxMEIiIi0sMEgYiIiPQwQSAdKpUK0dHRXIhF9Cf8d0F/R1ykSERERHpYQSAiIiI9TBCIiIhIDxMEIiIi0sME4W9AEAT8+OOP5X7+vn37IAgCcnJyntlnzpw5aNq0qUHjGdK3Q4cOiIiIMDhGotJe9HuvBH9/fyxduvS5feT82yGqSEwQFHT48GFYWFjgtddek/1cQ/5DYizDhg2DIAgQBAFWVlaoU6cOIiMj8fDhQ4PHiIyMxO7duxWLadOmTZg/f75i45HxmPP3vm/fvkZ9jeTkZLz77rvS47KSFqX/7TwLExGSiwmCglatWoVx48YhKSkJ165dM3U4srz22mvIzMzElStXsGDBAixbtgyRkZEGP9/BwQGurq6KxePi4gJHR0fFxiPjMefvvbG5ubnBzs7uuX2U/rdDpBQmCAp5+PAh/vOf/2D06NHo2bMnEhIS9Pps2bIFLVq0gI2NDWrUqIF+/foBeFJOz8jIwMSJE6W/5IGyM/6lS5fC399fepycnIwuXbqgRo0aUKvVCAkJwfHjx2XHr1Kp4OnpCV9fX4SHh2PQoEF6f+mkpKSgRYsWsLOzQ9u2bZGWliadKx3rvn370KpVK9jb26N69ep49dVXkZGRoTPeunXr4O/vD7VajYEDB+L+/fvSudJTDP7+/oiJicHw4cPh6OiIWrVq4YsvvtAZ7/Dhw2jatClsbGzQokUL/PjjjxAEAampqbI/DzKMuX/vn+fcuXN4/fXX4eDgAA8PDwwZMgR37tyRzt+/fx+DBg2Cvb09vLy8EB8fX+b39mmF5Gn8//znPyEIgvS49Pt9WtmIiYmBh4cHqlevjrlz56K4uBhTpkyBi4sLatasiVWrVunEO23aNNSrVw92dnaoU6cOZs2ahaKiIgBAQkIC5s6di5MnT0qf9dP/r3Jzc/Huu+/C3d0dTk5O6NSpE06ePKnoZ0nmiQmCQr799lvUr18f9evXx+DBg7F69Wr8eYuJn376Cf369UOPHj1w4sQJ7N69Gy1atADwpJxes2ZNzJs3D5mZmcjMzDT4de/fv4+hQ4fi4MGDOHLkCAIDA/H666/r/LItD1tbW+k/Lk/NnDkTixcvxrFjx2BpaYnhw4eX+dzi4mL07dsXISEhOHXqFH799Ve8++670i8AALh8+TJ+/PFHbNu2Ddu2bcP+/fvxr3/967kxLV68GC1atMCJEycwZswYjB49GhcuXJA+h169eiEoKAjHjx/H/PnzMW3atBf6DOivVbXv/VOZmZkICQlB06ZNcezYMezYsQN//PEHwsLCpD6TJk3CoUOHsGXLFiQmJuLgwYPPTVKSk5MBAKtXr0ZmZqb0uCx79uzBrVu3cODAASxZsgRz5sxBz5494ezsjN9++w3vvfce3nvvPVy/fl16jqOjIxISEnDu3Dl89NFH+PLLLxEfHw8AGDBgACZPnoyGDRtKn/WAAQMgiiJ69OgBjUaD7du3IyUlBc2aNUPnzp1x7969F/0YydyJpIi2bduKS5cuFUVRFIuKisQaNWqIiYmJ0vk2bdqIgwYNeubz/fz8xPj4eJ226OhosUmTJjpt8fHxop+f3zPHKS4uFh0dHcWtW7dKbQDEzZs3P/M5Q4cOFfv06SM9/u2330RXV1cxLCxMFEVR3Lt3rwhA3LVrl9Tnp59+EgGI+fn5erHevXtXBCDu27evzNeLjo4W7ezsxLy8PKltypQpYuvWraXHISEh4oQJE6THfn5+4uDBg6XHJSUloru7u7h8+XJRFEVx+fLloqurqxSPKIril19+KQIQT5w48cz3Ti+mKn3v/2zWrFli165dddquX78uAhDT0tLEvLw80crKSvzuu++k8zk5OaKdnZ3e9/bP76+smEq/36FDh4p+fn6iVquV2urXry+2a9dO5/3a29uLGzZseOb7i4uLE5s3b/7M1xFFUdy9e7fo5OQkPn78WKe9bt264ooVK545Nv09sIKggLS0NBw9ehQDBw4EAFhaWmLAgAE6JcDU1FR07txZ8dfOysrCe++9h3r16kGtVkOtVuPBgwey54K3bdsGBwcH2NjYoE2bNmjfvj0++eQTnT6NGzeWfvby8pJevzQXFxcMGzYM3bp1Q69evfDRRx/p/XXo7++vs8bAy8urzLGe9fqCIMDT01N6TlpaGho3bgwbGxupT6tWrf7qbdMLqArf+2dJSUnB3r174eDgIB0NGjQA8KT6deXKFRQVFel8x9RqNerXr6/I6zds2BDVqv3vP88eHh4ICgqSHltYWMDV1VXn38z333+Pf/zjH/D09ISDgwNmzZr1l59HSkoKHjx4AFdXV533mp6ejsuXLyvyXsh8WZo6gKpg5cqVKC4uho+Pj9QmiiKsrKyQnZ0NZ2dn2Nrayh63WrVqOuVaAHpl/2HDhuH27dtYunQp/Pz8oFKp0KZNGxQWFsp6rY4dO2L58uWwsrKCt7c3rKys9Pr8ue3pdEFJSUmZ461evRrjx4/Hjh078O233+KDDz5AYmIiXnnlFb2xno73rLHKev3SzxFFUWcK42kbGU9V+N4/S0lJCXr16oWFCxfqnfPy8sLFixcBwGjfubK+68/7/h85cgQDBw7E3Llz0a1bN6jVamzcuBGLFy9+7uuUlJTAy8sL+/bt0ztXvXr1F3oPZP5YQXhBxcXFWLt2LRYvXozU1FTpOHnyJPz8/PDNN98AePLX7/MuZbK2toZWq9Vpc3Nzg0aj0fmPTukFdwcPHsT48ePx+uuvo2HDhlCpVDoLqQxlb2+PgIAA+Pn5lZkclEdwcDCioqJw+PBhNGrUCOvXr1dk3LI0aNAAp06dQkFBgdR27Ngxo73e311V+d4/S7NmzXD27Fn4+/sjICBA57C3t0fdunVhZWWFo0ePSs/Jy8uTEodnsbKy0nu/Sjh06BD8/Pwwc+ZMtGjRAoGBgXqLgsv6rJs1awaNRgNLS0u991mjRg3F4yTzwgThBW3btg3Z2dkYMWIEGjVqpHO88cYbWLlyJQAgOjoaGzZsQHR0NM6fP4/Tp08jLi5OGsff3x8HDhzAzZs3pf/QdejQAbdv30ZcXBwuX76Mzz77DD///LPO6wcEBGDdunU4f/48fvvtNwwaNKhcf7UpKT09HVFRUfj111+RkZGBnTt34vfff8dLL71ktNcMDw9HSUkJ3n33XZw/fx6//PILFi1aBED/rzx6cVXle5+bm6uT4KSmpuLatWsYO3Ys7t27h7feegtHjx7FlStXsHPnTgwfPhxarRaOjo4YOnQopkyZgr179+Ls2bMYPnw4qlWr9tzvm7+/P3bv3g2NRoPs7GzZ8T5LQEAArl27ho0bN+Ly5cv4+OOPsXnzZr3XTk9PR2pqKu7cuYOCggKEhoaiTZs26Nu3L3755RdcvXoVhw8fxgcffMAEm5ggvKiVK1ciNDQUarVa71z//v2RmpqK48ePo0OHDvjuu++wZcsWNG3aFJ06dcJvv/0m9Z03bx6uXr2KunXrws3NDQDw0ksvYdmyZfjss8/QpEkTHD16VG9vglWrViE7OxvBwcEYMmQIxo8fD3d3d+O+6b9gZ2eHCxcuoH///qhXrx7effddvP/++xg1apTRXtPJyQlbt25FamoqmjZtipkzZ2L27NkAoLMugZRRVb73+/btQ3BwsM4xe/ZseHt749ChQ9BqtejWrRsaNWqECRMmQK1WS2sDlixZgjZt2qBnz54IDQ3Fq6++ipdeeum537fFixcjMTERvr6+CA4Olh3vs/Tp0wcTJ07E+++/j6ZNm+Lw4cOYNWuWTp/+/fvjtddeQ8eOHeHm5oYNGzZAEARs374d7du3x/Dhw1GvXj0MHDgQV69ehYeHh2LxkXni7Z6pyvrmm2/wzjvvIDc31+RVFar6Hj58CB8fHyxevBgjRowwdThEL4yLFKnKWLt2LerUqQMfHx+cPHkS06ZNQ1hYGJMDMooTJ07gwoULaNWqFXJzczFv3jwAT/6aJ6oKmCBQlaHRaDB79mxoNBp4eXnhzTffxIcffmjqsKgKW7RoEdLS0mBtbY3mzZvj4MGDXNxHVQanGIiIiEgPFykSERGRHiYIREREpIcJAhEREelhgkBERER6mCAQERGRHiYIREREpIcJAhEREelhgkBERER6mCAQERGRnv8PWsYnzrecQbMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Phishing', 'Actual Legitimate'], \n",
    "                                 index=['Predict Phishing', 'Predict Legitimate'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508cf7f",
   "metadata": {},
   "source": [
    "### Classification Report\n",
    "\n",
    "\n",
    "**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model.\n",
    "\n",
    "We can print a classification report as follows:-\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea00a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       561\n",
      "           1       0.96      0.94      0.95       571\n",
      "\n",
      "    accuracy                           0.95      1132\n",
      "   macro avg       0.95      0.95      0.95      1132\n",
      "weighted avg       0.95      0.95      0.95      1132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627a591",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "**Accuracy** measures the overall correctness of the predictions. It is a general indicator of how well the model is performing.\n",
    "\n",
    "Mathematically, accuracy can be defined as the ratio of `(TP + FP) to (TP + FP + FN + TN)`\n",
    "\n",
    "### Precision\n",
    "\n",
    "\n",
    "**Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP). \n",
    "\n",
    "\n",
    "So, **Precision** identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n",
    "\n",
    "\n",
    "\n",
    "Mathematically, precision can be defined as the ratio of `TP to (TP + FP)`.\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)\n",
    "\n",
    "### Recall\n",
    "\n",
    "\n",
    "Recall can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes.\n",
    "It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). **Recall** is also called **Sensitivity**.\n",
    "\n",
    "\n",
    "**Recall** identifies the proportion of correctly predicted actual positives.\n",
    "\n",
    "\n",
    "Mathematically, recall can be given as the ratio of `TP to (TP + FN)`.\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)\n",
    "\n",
    "### Specificity\n",
    "\n",
    "**Specificity** represents the propertion of correctly identified actual negatives. It helps us understand how well the model can idenitfy instances that don't belong the the positive class. High specificity is indicative that the model is good at avoiding false positives. Low specificity is indicative that the model is misclassifying negative instances as positive.\n",
    "\n",
    "Mathematically, specificity can be given as the ratio of `TN to (TN + FP)`\n",
    "\n",
    "### Negative Predictive Value (NPV)\n",
    "**NPV** asses the likelihood that a negative prediction is correct. High NPV indicates the model is correctly idenitfying true negatives. Low NPV indicates the model is missclassifying negative instances.\n",
    "\n",
    "Mathematically, NPV can be given as the ratio of `TN to (TN + FN)`\n",
    "\n",
    "### f1-score\n",
    "\n",
    "\n",
    "**f1-score** is the weighted harmonic mean of precision and recall. The best possible **f1-score** would be 1.0 and the worst \n",
    "would be 0.0.  **f1-score** is the harmonic mean of precision and recall. So, **f1-score** is always lower than accuracy measures as they embed precision and recall into their computation. The weighted average of `f1-score` should be used to \n",
    "compare classifier models, not global accuracy.\n",
    "\n",
    "Mathematically, f1-score can be given by the following formula: `2 x (Percision x Recall)/(Precision + Recall)`\n",
    "\n",
    "#### from Phrashant Banerjee [Naive Bayes Classifier in Python](https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "443b0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9496\n",
      "Precision : 0.9554\n",
      "Recall: 0.9437\n",
      "Specificity : 0.9557\n",
      "NPV: 0.9440\n",
      "F1 Score: 0.9495\n",
      "Balanced Accuracy: 0.9497\n"
     ]
    }
   ],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "negative_predictive_value = TN / (TN + FN)\n",
    "accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "f1_score = 2*((precision * recall) / (precision + recall))\n",
    "balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "print('Accuracy : {0:0.4f}'.format(accuracy))\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "print('Recall: {0:0.4f}'.format(recall))\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "print('NPV: {0:0.4f}'.format(negative_predictive_value))\n",
    "print('F1 Score: {0:0.4f}'.format(f1_score))\n",
    "print('Balanced Accuracy: {0:0.4f}'.format(balanced_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72b7dc",
   "metadata": {},
   "source": [
    "# **7. Improvements** <a class=\"anchor\" id=\"7\"></a>\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Now that we've evaluated our model, we can reflect on our result and see if there are any improvements to be made.\n",
    "\n",
    "Given our results of `94.7%`, we can conclude that our model is exceedingly well trained to accurately predicts if urls are legitimate or phishing. From this point forward, our goal would be to try and train the model on fewer attributes to see if we can maintain accuracy with less data. 87 attributes is a lot of attributes to make predictions, and it would be more useful for our use-case to reduce the number of attributes, especially if we can maintain a high level of accuracy.\n",
    "\n",
    "If we want to implement this model in a useful way in the future, it is easier for us to work with attributes that are easily obtainable from the url. Such attributes include the following:\n",
    "\n",
    "- length_url\n",
    "- length_hostname\n",
    "- nb_dots\n",
    "- nb_hyphens\n",
    "- nb_at\n",
    "- nb_qm\n",
    "- nb_and\n",
    "- nb_or\n",
    "- nb_eq\n",
    "- nb_underscore\n",
    "- nb_tilde\n",
    "- nb_percent\n",
    "- nb_slash\n",
    "- nb_star\n",
    "- nb_colon\n",
    "- nb_comma\n",
    "- nb_semicolumn\n",
    "- nb_dollar\n",
    "- nb_space\n",
    "- nb_www\n",
    "- nb_com\n",
    "- nb_dslash\n",
    "- http_in_path\n",
    "- nb_subdomains\n",
    "\n",
    "These attributes can be directly derives from the url string, making them especially useful in a real-world application of our model if we can train it accurately on just these attributes. This would also bring our total attributes down from 87 to 24, meaning we need less data to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fa5513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_working_directory = os.getcwd()\n",
    "file = r'\\input\\phishingURL.parquet' #this directory may differ for you\n",
    "data = current_working_directory + file\n",
    "\n",
    "# convert to a csv\n",
    "df = pd.read_parquet(data)\n",
    "df.to_csv(current_working_directory + r'\\input\\phishingURL.csv') \n",
    "\n",
    "# read the new csv\n",
    "data = current_working_directory + r'\\input\\phishingURL.csv'\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2bfcfbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>nb_underscore</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_comma</th>\n",
       "      <th>nb_semicolumn</th>\n",
       "      <th>nb_dollar</th>\n",
       "      <th>nb_space</th>\n",
       "      <th>nb_www</th>\n",
       "      <th>nb_com</th>\n",
       "      <th>nb_dslash</th>\n",
       "      <th>http_in_path</th>\n",
       "      <th>nb_subdomains</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>550</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>legitimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>77</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3772 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length_url  length_hostname  nb_dots  nb_hyphens  nb_at  nb_qm  nb_and  \\\n",
       "0             36               19        2           0      0      0       0   \n",
       "1             51               24        3           0      0      0       0   \n",
       "2             46               16        2           0      0      0       0   \n",
       "3            185               17        2           1      0      1       2   \n",
       "4             52               16        2           0      0      0       0   \n",
       "...          ...              ...      ...         ...    ...    ...     ...   \n",
       "3767          34               26        2           0      0      0       0   \n",
       "3768          54               14        2           0      0      0       0   \n",
       "3769          25               16        2           0      0      0       0   \n",
       "3770         550               25        5          24      0      1       9   \n",
       "3771          77               23        1           0      0      0       0   \n",
       "\n",
       "      nb_or  nb_eq  nb_underscore  ...  nb_comma  nb_semicolumn  nb_dollar  \\\n",
       "0         0      0              0  ...         0              0          0   \n",
       "1         0      0              0  ...         0              0          0   \n",
       "2         0      0              2  ...         0              0          0   \n",
       "3         0      3              2  ...         0              2          0   \n",
       "4         0      0              1  ...         0              0          0   \n",
       "...     ...    ...            ...  ...       ...            ...        ...   \n",
       "3767      0      0              0  ...         0              0          0   \n",
       "3768      0      0              2  ...         0              0          0   \n",
       "3769      0      0              0  ...         0              0          0   \n",
       "3770      0     10              7  ...         0              0          0   \n",
       "3771      0      0              0  ...         0              0          0   \n",
       "\n",
       "      nb_space  nb_www  nb_com  nb_dslash  http_in_path  nb_subdomains  \\\n",
       "0            0       0       0          0             0              2   \n",
       "1            0       1       0          0             0              3   \n",
       "2            0       0       0          0             0              2   \n",
       "3            0       0       0          0             0              2   \n",
       "4            0       0       0          0             0              2   \n",
       "...        ...     ...     ...        ...           ...            ...   \n",
       "3767         0       1       0          0             0              2   \n",
       "3768         0       0       0          0             0              2   \n",
       "3769         0       1       0          0             0              2   \n",
       "3770         1       0       1          0             1              3   \n",
       "3771         0       0       0          0             0              1   \n",
       "\n",
       "          status  \n",
       "0       phishing  \n",
       "1     legitimate  \n",
       "2     legitimate  \n",
       "3       phishing  \n",
       "4     legitimate  \n",
       "...          ...  \n",
       "3767  legitimate  \n",
       "3768  legitimate  \n",
       "3769  legitimate  \n",
       "3770  legitimate  \n",
       "3771    phishing  \n",
       "\n",
       "[3772 rows x 25 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep = [\"length_url\", \"length_hostname\", \"nb_dots\", \"nb_hyphens\", \"nb_at\", \"nb_qm\", \n",
    "                \"nb_and\", \"nb_or\", \"nb_eq\", \"nb_underscore\", \"nb_tilde\", \"nb_percent\", \"nb_slash\", \n",
    "                \"nb_star\", \"nb_colon\", \"nb_comma\", \"nb_semicolumn\", \"nb_dollar\", \"nb_space\", \"nb_www\", \n",
    "                \"nb_com\", \"nb_dslash\", \"http_in_path\", \"nb_subdomains\", \"status\"]\n",
    "\n",
    "# Filter out columns not present in the column_names list\n",
    "columns_to_keep = [col for col in df.columns if col in cols_to_keep]\n",
    "\n",
    "# Drop columns not present in the column_names list\n",
    "df = df[cols_to_keep]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3360bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>nb_underscore</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_colon</th>\n",
       "      <th>nb_comma</th>\n",
       "      <th>nb_semicolumn</th>\n",
       "      <th>nb_dollar</th>\n",
       "      <th>nb_space</th>\n",
       "      <th>nb_www</th>\n",
       "      <th>nb_com</th>\n",
       "      <th>nb_dslash</th>\n",
       "      <th>http_in_path</th>\n",
       "      <th>nb_subdomains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>54</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>550</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>77</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3772 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length_url  length_hostname  nb_dots  nb_hyphens  nb_at  nb_qm  nb_and  \\\n",
       "0             36               19        2           0      0      0       0   \n",
       "1             51               24        3           0      0      0       0   \n",
       "2             46               16        2           0      0      0       0   \n",
       "3            185               17        2           1      0      1       2   \n",
       "4             52               16        2           0      0      0       0   \n",
       "...          ...              ...      ...         ...    ...    ...     ...   \n",
       "3767          34               26        2           0      0      0       0   \n",
       "3768          54               14        2           0      0      0       0   \n",
       "3769          25               16        2           0      0      0       0   \n",
       "3770         550               25        5          24      0      1       9   \n",
       "3771          77               23        1           0      0      0       0   \n",
       "\n",
       "      nb_or  nb_eq  nb_underscore  ...  nb_colon  nb_comma  nb_semicolumn  \\\n",
       "0         0      0              0  ...         1         0              0   \n",
       "1         0      0              0  ...         1         0              0   \n",
       "2         0      0              2  ...         1         0              0   \n",
       "3         0      3              2  ...         1         0              2   \n",
       "4         0      0              1  ...         1         0              0   \n",
       "...     ...    ...            ...  ...       ...       ...            ...   \n",
       "3767      0      0              0  ...         1         0              0   \n",
       "3768      0      0              2  ...         2         0              0   \n",
       "3769      0      0              0  ...         1         0              0   \n",
       "3770      0     10              7  ...         2         0              0   \n",
       "3771      0      0              0  ...         1         0              0   \n",
       "\n",
       "      nb_dollar  nb_space  nb_www  nb_com  nb_dslash  http_in_path  \\\n",
       "0             0         0       0       0          0             0   \n",
       "1             0         0       1       0          0             0   \n",
       "2             0         0       0       0          0             0   \n",
       "3             0         0       0       0          0             0   \n",
       "4             0         0       0       0          0             0   \n",
       "...         ...       ...     ...     ...        ...           ...   \n",
       "3767          0         0       1       0          0             0   \n",
       "3768          0         0       0       0          0             0   \n",
       "3769          0         0       1       0          0             0   \n",
       "3770          0         1       0       1          0             1   \n",
       "3771          0         0       0       0          0             0   \n",
       "\n",
       "      nb_subdomains  \n",
       "0                 2  \n",
       "1                 3  \n",
       "2                 2  \n",
       "3                 2  \n",
       "4                 2  \n",
       "...             ...  \n",
       "3767              2  \n",
       "3768              2  \n",
       "3769              2  \n",
       "3770              3  \n",
       "3771              1  \n",
       "\n",
       "[3772 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select our target variable\n",
    "target_variable = 'status'\n",
    "\n",
    "X_target = df.drop([target_variable], axis=1)\n",
    "\n",
    "y = df[target_variable]\n",
    "df.drop([target_variable], inplace=True, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5f6188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_url</th>\n",
       "      <th>length_hostname</th>\n",
       "      <th>nb_dots</th>\n",
       "      <th>nb_hyphens</th>\n",
       "      <th>nb_at</th>\n",
       "      <th>nb_qm</th>\n",
       "      <th>nb_and</th>\n",
       "      <th>nb_or</th>\n",
       "      <th>nb_eq</th>\n",
       "      <th>nb_underscore</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_colon</th>\n",
       "      <th>nb_comma</th>\n",
       "      <th>nb_semicolumn</th>\n",
       "      <th>nb_dollar</th>\n",
       "      <th>nb_space</th>\n",
       "      <th>nb_www</th>\n",
       "      <th>nb_com</th>\n",
       "      <th>nb_dslash</th>\n",
       "      <th>http_in_path</th>\n",
       "      <th>nb_subdomains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.530043</td>\n",
       "      <td>-0.192143</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>-0.302916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.216023</td>\n",
       "      <td>0.229918</td>\n",
       "      <td>0.365720</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>-0.302916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>1.107629</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>1.182621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.320696</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>1.650244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.589220</td>\n",
       "      <td>-0.360967</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>2.391141</td>\n",
       "      <td>2.310396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.843696</td>\n",
       "      <td>1.650244</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>3.227304</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.195088</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>0.673664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>-0.571912</td>\n",
       "      <td>0.398743</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>-0.302916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>1.107629</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3768</th>\n",
       "      <td>-0.153219</td>\n",
       "      <td>-0.614204</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>1.650244</td>\n",
       "      <td>...</td>\n",
       "      <td>4.655183</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>-0.760324</td>\n",
       "      <td>-0.445380</td>\n",
       "      <td>-0.374353</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>-0.302916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>1.107629</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-0.389207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3770</th>\n",
       "      <td>10.230366</td>\n",
       "      <td>0.314330</td>\n",
       "      <td>1.845867</td>\n",
       "      <td>10.709067</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>2.391141</td>\n",
       "      <td>11.138717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.196315</td>\n",
       "      <td>6.533142</td>\n",
       "      <td>...</td>\n",
       "      <td>4.655183</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>2.495487</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>2.36329</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>6.760640</td>\n",
       "      <td>1.182621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>0.328278</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>-1.114427</td>\n",
       "      <td>-0.458789</td>\n",
       "      <td>-0.151506</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>-0.211981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307427</td>\n",
       "      <td>-0.302916</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.113788</td>\n",
       "      <td>-0.036066</td>\n",
       "      <td>-0.110157</td>\n",
       "      <td>-0.030908</td>\n",
       "      <td>-0.117777</td>\n",
       "      <td>-0.887584</td>\n",
       "      <td>-0.35201</td>\n",
       "      <td>-0.080021</td>\n",
       "      <td>-0.092661</td>\n",
       "      <td>-1.961034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3772 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      length_url  length_hostname   nb_dots  nb_hyphens     nb_at     nb_qm  \\\n",
       "0      -0.530043        -0.192143 -0.374353   -0.458789 -0.151506 -0.384054   \n",
       "1      -0.216023         0.229918  0.365720   -0.458789 -0.151506 -0.384054   \n",
       "2      -0.320696        -0.445380 -0.374353   -0.458789 -0.151506 -0.384054   \n",
       "3       2.589220        -0.360967 -0.374353    0.006538 -0.151506  2.391141   \n",
       "4      -0.195088        -0.445380 -0.374353   -0.458789 -0.151506 -0.384054   \n",
       "...          ...              ...       ...         ...       ...       ...   \n",
       "3767   -0.571912         0.398743 -0.374353   -0.458789 -0.151506 -0.384054   \n",
       "3768   -0.153219        -0.614204 -0.374353   -0.458789 -0.151506 -0.384054   \n",
       "3769   -0.760324        -0.445380 -0.374353   -0.458789 -0.151506 -0.384054   \n",
       "3770   10.230366         0.314330  1.845867   10.709067 -0.151506  2.391141   \n",
       "3771    0.328278         0.145506 -1.114427   -0.458789 -0.151506 -0.384054   \n",
       "\n",
       "         nb_and  nb_or      nb_eq  nb_underscore  ...  nb_colon  nb_comma  \\\n",
       "0     -0.211981    0.0  -0.307427      -0.302916  ... -0.113788 -0.036066   \n",
       "1     -0.211981    0.0  -0.307427      -0.302916  ... -0.113788 -0.036066   \n",
       "2     -0.211981    0.0  -0.307427       1.650244  ... -0.113788 -0.036066   \n",
       "3      2.310396    0.0   2.843696       1.650244  ... -0.113788 -0.036066   \n",
       "4     -0.211981    0.0  -0.307427       0.673664  ... -0.113788 -0.036066   \n",
       "...         ...    ...        ...            ...  ...       ...       ...   \n",
       "3767  -0.211981    0.0  -0.307427      -0.302916  ... -0.113788 -0.036066   \n",
       "3768  -0.211981    0.0  -0.307427       1.650244  ...  4.655183 -0.036066   \n",
       "3769  -0.211981    0.0  -0.307427      -0.302916  ... -0.113788 -0.036066   \n",
       "3770  11.138717    0.0  10.196315       6.533142  ...  4.655183 -0.036066   \n",
       "3771  -0.211981    0.0  -0.307427      -0.302916  ... -0.113788 -0.036066   \n",
       "\n",
       "      nb_semicolumn  nb_dollar  nb_space    nb_www   nb_com  nb_dslash  \\\n",
       "0         -0.110157  -0.030908 -0.117777 -0.887584 -0.35201  -0.080021   \n",
       "1         -0.110157  -0.030908 -0.117777  1.107629 -0.35201  -0.080021   \n",
       "2         -0.110157  -0.030908 -0.117777 -0.887584 -0.35201  -0.080021   \n",
       "3          3.227304  -0.030908 -0.117777 -0.887584 -0.35201  -0.080021   \n",
       "4         -0.110157  -0.030908 -0.117777 -0.887584 -0.35201  -0.080021   \n",
       "...             ...        ...       ...       ...      ...        ...   \n",
       "3767      -0.110157  -0.030908 -0.117777  1.107629 -0.35201  -0.080021   \n",
       "3768      -0.110157  -0.030908 -0.117777 -0.887584 -0.35201  -0.080021   \n",
       "3769      -0.110157  -0.030908 -0.117777  1.107629 -0.35201  -0.080021   \n",
       "3770      -0.110157  -0.030908  2.495487 -0.887584  2.36329  -0.080021   \n",
       "3771      -0.110157  -0.030908 -0.117777 -0.887584 -0.35201  -0.080021   \n",
       "\n",
       "      http_in_path  nb_subdomains  \n",
       "0        -0.092661      -0.389207  \n",
       "1        -0.092661       1.182621  \n",
       "2        -0.092661      -0.389207  \n",
       "3        -0.092661      -0.389207  \n",
       "4        -0.092661      -0.389207  \n",
       "...            ...            ...  \n",
       "3767     -0.092661      -0.389207  \n",
       "3768     -0.092661      -0.389207  \n",
       "3769     -0.092661      -0.389207  \n",
       "3770      6.760640       1.182621  \n",
       "3771     -0.092661      -1.961034  \n",
       "\n",
       "[3772 rows x 24 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(data=X, columns=df.columns)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80492fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\n",
    "\n",
    "# encode the training sets\n",
    "y_train = (y_train == 'legitimate').astype(int)\n",
    "y_test = (y_test == 'legitimate').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c47ed7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "fnn_model = Sequential()\n",
    "\n",
    "# input layer neruons count equal to number of features and ReLU activation\n",
    "input_layer = Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu')\n",
    "\n",
    "# hidden layer with 64 neurons for the first, and 32 for the second, and ReLU activation\n",
    "hidden_layer_one = Dense(64, activation = 'relu')\n",
    "hidden_layer_two = Dense(32, activation = 'relu')\n",
    "\n",
    "# output layer with 1 neuron and sigmoid activation (binary classification)\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "# add the layers to the model\n",
    "fnn_model.add(input_layer)\n",
    "fnn_model.add(hidden_layer_one)\n",
    "fnn_model.add(hidden_layer_two)\n",
    "fnn_model.add(output_layer)\n",
    "\n",
    "# compile the model\n",
    "fnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c099d0c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6913 - loss: 0.6127 - val_accuracy: 0.7959 - val_loss: 0.4419\n",
      "Epoch 2/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8023 - loss: 0.4285 - val_accuracy: 0.8004 - val_loss: 0.4137\n",
      "Epoch 3/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 840us/step - accuracy: 0.8106 - loss: 0.3999 - val_accuracy: 0.8065 - val_loss: 0.4029\n",
      "Epoch 4/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.8225 - loss: 0.3741 - val_accuracy: 0.8030 - val_loss: 0.4042\n",
      "Epoch 5/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8208 - loss: 0.3924 - val_accuracy: 0.8057 - val_loss: 0.4001\n",
      "Epoch 6/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8315 - loss: 0.3678 - val_accuracy: 0.8083 - val_loss: 0.3988\n",
      "Epoch 7/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - accuracy: 0.8411 - loss: 0.3625 - val_accuracy: 0.8074 - val_loss: 0.3982\n",
      "Epoch 8/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8458 - loss: 0.3485 - val_accuracy: 0.8118 - val_loss: 0.3966\n",
      "Epoch 9/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8382 - loss: 0.3580 - val_accuracy: 0.8065 - val_loss: 0.4142\n",
      "Epoch 10/10\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8299 - loss: 0.3669 - val_accuracy: 0.8145 - val_loss: 0.3991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2cd6a9a2af0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd1905",
   "metadata": {},
   "source": [
    "We can already see that our accuracy is much lower than our previous model. However, we can train our model for more epochs to improve it's accuracy. Let's rerun our model for longer and see how this improves our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a367b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnn_model = Sequential()\n",
    "\n",
    "# input layer neruons count equal to number of features and ReLU activation\n",
    "input_layer = Dense(X_train.shape[1], input_dim=X_train.shape[1], activation='relu')\n",
    "\n",
    "# hidden layer with 64 neurons for the first, and 32 for the second, and ReLU activation\n",
    "hidden_layer_one = Dense(64, activation = 'relu')\n",
    "hidden_layer_two = Dense(32, activation = 'relu')\n",
    "\n",
    "# output layer with 1 neuron and sigmoid activation (binary classification)\n",
    "output_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "# add the layers to the model\n",
    "fnn_model.add(input_layer)\n",
    "fnn_model.add(hidden_layer_one)\n",
    "fnn_model.add(hidden_layer_two)\n",
    "fnn_model.add(output_layer)\n",
    "\n",
    "# compile the model\n",
    "fnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e0d7d3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6916 - loss: 0.6155 - val_accuracy: 0.7588 - val_loss: 0.4870\n",
      "Epoch 2/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.7919 - loss: 0.4505 - val_accuracy: 0.7995 - val_loss: 0.4408\n",
      "Epoch 3/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8140 - loss: 0.4032 - val_accuracy: 0.8057 - val_loss: 0.4202\n",
      "Epoch 4/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8323 - loss: 0.3881 - val_accuracy: 0.8083 - val_loss: 0.4166\n",
      "Epoch 5/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 810us/step - accuracy: 0.8224 - loss: 0.3827 - val_accuracy: 0.8127 - val_loss: 0.4048\n",
      "Epoch 6/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8334 - loss: 0.3677 - val_accuracy: 0.8057 - val_loss: 0.4114\n",
      "Epoch 7/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8297 - loss: 0.3619 - val_accuracy: 0.8127 - val_loss: 0.4018\n",
      "Epoch 8/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8470 - loss: 0.3416 - val_accuracy: 0.8092 - val_loss: 0.4053\n",
      "Epoch 9/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8322 - loss: 0.3611 - val_accuracy: 0.8057 - val_loss: 0.4008\n",
      "Epoch 10/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.8457 - loss: 0.3363 - val_accuracy: 0.8118 - val_loss: 0.3978\n",
      "Epoch 11/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8465 - loss: 0.3375 - val_accuracy: 0.8118 - val_loss: 0.4003\n",
      "Epoch 12/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 841us/step - accuracy: 0.8467 - loss: 0.3386 - val_accuracy: 0.8171 - val_loss: 0.3928\n",
      "Epoch 13/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8444 - loss: 0.3282 - val_accuracy: 0.8145 - val_loss: 0.4023\n",
      "Epoch 14/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8452 - loss: 0.3404 - val_accuracy: 0.8224 - val_loss: 0.3919\n",
      "Epoch 15/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8508 - loss: 0.3305 - val_accuracy: 0.8171 - val_loss: 0.3936\n",
      "Epoch 16/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8466 - loss: 0.3306 - val_accuracy: 0.8260 - val_loss: 0.3912\n",
      "Epoch 17/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8580 - loss: 0.3145 - val_accuracy: 0.8189 - val_loss: 0.3911\n",
      "Epoch 18/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8588 - loss: 0.3134 - val_accuracy: 0.8136 - val_loss: 0.4033\n",
      "Epoch 19/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.8383 - loss: 0.3355 - val_accuracy: 0.8224 - val_loss: 0.3922\n",
      "Epoch 20/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.8628 - loss: 0.3114 - val_accuracy: 0.8313 - val_loss: 0.3854\n",
      "Epoch 21/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.8543 - loss: 0.3131 - val_accuracy: 0.8313 - val_loss: 0.3926\n",
      "Epoch 22/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.8558 - loss: 0.3093 - val_accuracy: 0.8242 - val_loss: 0.3840\n",
      "Epoch 23/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.8664 - loss: 0.3002 - val_accuracy: 0.8269 - val_loss: 0.3887\n",
      "Epoch 24/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8594 - loss: 0.3074 - val_accuracy: 0.8277 - val_loss: 0.3856\n",
      "Epoch 25/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8638 - loss: 0.2932 - val_accuracy: 0.8216 - val_loss: 0.3949\n",
      "Epoch 26/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 857us/step - accuracy: 0.8617 - loss: 0.2987 - val_accuracy: 0.8304 - val_loss: 0.3886\n",
      "Epoch 27/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.8718 - loss: 0.2794 - val_accuracy: 0.8322 - val_loss: 0.3900\n",
      "Epoch 28/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8609 - loss: 0.3093 - val_accuracy: 0.8277 - val_loss: 0.4106\n",
      "Epoch 29/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8600 - loss: 0.3003 - val_accuracy: 0.8348 - val_loss: 0.3975\n",
      "Epoch 30/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8764 - loss: 0.2824 - val_accuracy: 0.8286 - val_loss: 0.3906\n",
      "Epoch 31/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8542 - loss: 0.3085 - val_accuracy: 0.8313 - val_loss: 0.3963\n",
      "Epoch 32/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8806 - loss: 0.2759 - val_accuracy: 0.8375 - val_loss: 0.3941\n",
      "Epoch 33/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 823us/step - accuracy: 0.8788 - loss: 0.2912 - val_accuracy: 0.8313 - val_loss: 0.3960\n",
      "Epoch 34/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8638 - loss: 0.3021 - val_accuracy: 0.8357 - val_loss: 0.3940\n",
      "Epoch 35/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822us/step - accuracy: 0.8759 - loss: 0.2851 - val_accuracy: 0.8295 - val_loss: 0.4086\n",
      "Epoch 36/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8656 - loss: 0.2939 - val_accuracy: 0.8357 - val_loss: 0.3948\n",
      "Epoch 37/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8694 - loss: 0.2939 - val_accuracy: 0.8383 - val_loss: 0.3972\n",
      "Epoch 38/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.8693 - loss: 0.2922 - val_accuracy: 0.8251 - val_loss: 0.3976\n",
      "Epoch 39/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8811 - loss: 0.2776 - val_accuracy: 0.8233 - val_loss: 0.3959\n",
      "Epoch 40/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8678 - loss: 0.2876 - val_accuracy: 0.8322 - val_loss: 0.4059\n",
      "Epoch 41/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.8806 - loss: 0.2725 - val_accuracy: 0.8304 - val_loss: 0.4136\n",
      "Epoch 42/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8794 - loss: 0.2682 - val_accuracy: 0.8322 - val_loss: 0.4086\n",
      "Epoch 43/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8854 - loss: 0.2643 - val_accuracy: 0.8375 - val_loss: 0.4060\n",
      "Epoch 44/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8841 - loss: 0.2631 - val_accuracy: 0.8286 - val_loss: 0.4184\n",
      "Epoch 45/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8756 - loss: 0.2774 - val_accuracy: 0.8330 - val_loss: 0.4043\n",
      "Epoch 46/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8716 - loss: 0.2750 - val_accuracy: 0.8375 - val_loss: 0.3994\n",
      "Epoch 47/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8700 - loss: 0.2696 - val_accuracy: 0.8313 - val_loss: 0.4110\n",
      "Epoch 48/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8866 - loss: 0.2573 - val_accuracy: 0.8410 - val_loss: 0.3994\n",
      "Epoch 49/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - accuracy: 0.8819 - loss: 0.2597 - val_accuracy: 0.8339 - val_loss: 0.4089\n",
      "Epoch 50/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.8827 - loss: 0.2524 - val_accuracy: 0.8357 - val_loss: 0.4101\n",
      "Epoch 51/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - accuracy: 0.8805 - loss: 0.2621 - val_accuracy: 0.8366 - val_loss: 0.4125\n",
      "Epoch 52/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8706 - loss: 0.2724 - val_accuracy: 0.8313 - val_loss: 0.4261\n",
      "Epoch 53/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8839 - loss: 0.2457 - val_accuracy: 0.8295 - val_loss: 0.4162\n",
      "Epoch 54/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8914 - loss: 0.2591 - val_accuracy: 0.8419 - val_loss: 0.4126\n",
      "Epoch 55/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.8797 - loss: 0.2610 - val_accuracy: 0.8304 - val_loss: 0.4220\n",
      "Epoch 56/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 832us/step - accuracy: 0.8716 - loss: 0.2571 - val_accuracy: 0.8339 - val_loss: 0.4171\n",
      "Epoch 57/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.8796 - loss: 0.2602 - val_accuracy: 0.8233 - val_loss: 0.4277\n",
      "Epoch 58/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8853 - loss: 0.2565 - val_accuracy: 0.8401 - val_loss: 0.4287\n",
      "Epoch 59/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8811 - loss: 0.2613 - val_accuracy: 0.8260 - val_loss: 0.4249\n",
      "Epoch 60/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8910 - loss: 0.2415 - val_accuracy: 0.8224 - val_loss: 0.4330\n",
      "Epoch 61/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 859us/step - accuracy: 0.8806 - loss: 0.2515 - val_accuracy: 0.8260 - val_loss: 0.4455\n",
      "Epoch 62/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.8885 - loss: 0.2483 - val_accuracy: 0.8224 - val_loss: 0.4402\n",
      "Epoch 63/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.8848 - loss: 0.2472 - val_accuracy: 0.8216 - val_loss: 0.4563\n",
      "Epoch 64/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8941 - loss: 0.2437 - val_accuracy: 0.8330 - val_loss: 0.4346\n",
      "Epoch 65/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8860 - loss: 0.2451 - val_accuracy: 0.8383 - val_loss: 0.4338\n",
      "Epoch 66/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8941 - loss: 0.2409 - val_accuracy: 0.8286 - val_loss: 0.4309\n",
      "Epoch 67/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.9006 - loss: 0.2376 - val_accuracy: 0.8339 - val_loss: 0.4445\n",
      "Epoch 68/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - accuracy: 0.8945 - loss: 0.2367 - val_accuracy: 0.8286 - val_loss: 0.4527\n",
      "Epoch 69/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.9038 - loss: 0.2315 - val_accuracy: 0.8313 - val_loss: 0.4452\n",
      "Epoch 70/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.8908 - loss: 0.2404 - val_accuracy: 0.8251 - val_loss: 0.4385\n",
      "Epoch 71/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.8805 - loss: 0.2507 - val_accuracy: 0.8366 - val_loss: 0.4309\n",
      "Epoch 72/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.8961 - loss: 0.2309 - val_accuracy: 0.8330 - val_loss: 0.4563\n",
      "Epoch 73/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.8952 - loss: 0.2399 - val_accuracy: 0.8313 - val_loss: 0.4614\n",
      "Epoch 74/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8846 - loss: 0.2521 - val_accuracy: 0.8269 - val_loss: 0.4770\n",
      "Epoch 75/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.8899 - loss: 0.2296 - val_accuracy: 0.8216 - val_loss: 0.4526\n",
      "Epoch 76/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.8906 - loss: 0.2429 - val_accuracy: 0.8392 - val_loss: 0.4607\n",
      "Epoch 77/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.8877 - loss: 0.2630 - val_accuracy: 0.8171 - val_loss: 0.4516\n",
      "Epoch 78/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.8902 - loss: 0.2351 - val_accuracy: 0.8339 - val_loss: 0.4430\n",
      "Epoch 79/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.8946 - loss: 0.2356 - val_accuracy: 0.8357 - val_loss: 0.4481\n",
      "Epoch 80/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - accuracy: 0.9001 - loss: 0.2198 - val_accuracy: 0.8277 - val_loss: 0.4638\n",
      "Epoch 81/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8997 - loss: 0.2353 - val_accuracy: 0.8260 - val_loss: 0.4522\n",
      "Epoch 82/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8947 - loss: 0.2338 - val_accuracy: 0.8216 - val_loss: 0.4540\n",
      "Epoch 83/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 880us/step - accuracy: 0.8910 - loss: 0.2334 - val_accuracy: 0.8224 - val_loss: 0.4609\n",
      "Epoch 84/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.9019 - loss: 0.2193 - val_accuracy: 0.8269 - val_loss: 0.4629\n",
      "Epoch 85/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - accuracy: 0.8961 - loss: 0.2366 - val_accuracy: 0.8198 - val_loss: 0.4531\n",
      "Epoch 86/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8919 - loss: 0.2351 - val_accuracy: 0.8269 - val_loss: 0.4565\n",
      "Epoch 87/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8919 - loss: 0.2394 - val_accuracy: 0.8313 - val_loss: 0.4741\n",
      "Epoch 88/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8931 - loss: 0.2318 - val_accuracy: 0.8233 - val_loss: 0.4856\n",
      "Epoch 89/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8963 - loss: 0.2348 - val_accuracy: 0.8322 - val_loss: 0.4623\n",
      "Epoch 90/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.8993 - loss: 0.2221 - val_accuracy: 0.8189 - val_loss: 0.4716\n",
      "Epoch 91/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.8876 - loss: 0.2468 - val_accuracy: 0.8127 - val_loss: 0.4862\n",
      "Epoch 92/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - accuracy: 0.8830 - loss: 0.2401 - val_accuracy: 0.8269 - val_loss: 0.4913\n",
      "Epoch 93/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 893us/step - accuracy: 0.9019 - loss: 0.2213 - val_accuracy: 0.8260 - val_loss: 0.4829\n",
      "Epoch 94/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 894us/step - accuracy: 0.8935 - loss: 0.2223 - val_accuracy: 0.8251 - val_loss: 0.4744\n",
      "Epoch 95/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 896us/step - accuracy: 0.8942 - loss: 0.2346 - val_accuracy: 0.8339 - val_loss: 0.4753\n",
      "Epoch 96/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9045 - loss: 0.2123 - val_accuracy: 0.8277 - val_loss: 0.4927\n",
      "Epoch 97/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.8970 - loss: 0.2267 - val_accuracy: 0.8277 - val_loss: 0.4706\n",
      "Epoch 98/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.8916 - loss: 0.2274 - val_accuracy: 0.8224 - val_loss: 0.4929\n",
      "Epoch 99/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.8905 - loss: 0.2391 - val_accuracy: 0.8348 - val_loss: 0.4763\n",
      "Epoch 100/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.8974 - loss: 0.2148 - val_accuracy: 0.8242 - val_loss: 0.4912\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.8881 - loss: 0.2219 - val_accuracy: 0.8198 - val_loss: 0.4802\n",
      "Epoch 102/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.9045 - loss: 0.2027 - val_accuracy: 0.8322 - val_loss: 0.4878\n",
      "Epoch 103/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870us/step - accuracy: 0.9113 - loss: 0.2073 - val_accuracy: 0.8136 - val_loss: 0.5163\n",
      "Epoch 104/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8952 - loss: 0.2285 - val_accuracy: 0.8366 - val_loss: 0.4840\n",
      "Epoch 105/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.9043 - loss: 0.2039 - val_accuracy: 0.8269 - val_loss: 0.5016\n",
      "Epoch 106/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.8953 - loss: 0.2238 - val_accuracy: 0.8260 - val_loss: 0.5027\n",
      "Epoch 107/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9080 - loss: 0.2119 - val_accuracy: 0.8198 - val_loss: 0.4843\n",
      "Epoch 108/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.8970 - loss: 0.2165 - val_accuracy: 0.8322 - val_loss: 0.4831\n",
      "Epoch 109/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.9030 - loss: 0.2178 - val_accuracy: 0.8242 - val_loss: 0.5226\n",
      "Epoch 110/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.9003 - loss: 0.2199 - val_accuracy: 0.8154 - val_loss: 0.4988\n",
      "Epoch 111/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928us/step - accuracy: 0.9009 - loss: 0.2047 - val_accuracy: 0.8277 - val_loss: 0.4964\n",
      "Epoch 112/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 931us/step - accuracy: 0.8983 - loss: 0.2247 - val_accuracy: 0.8127 - val_loss: 0.5358\n",
      "Epoch 113/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9050 - loss: 0.2120 - val_accuracy: 0.8224 - val_loss: 0.5140\n",
      "Epoch 114/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.9063 - loss: 0.2077 - val_accuracy: 0.8348 - val_loss: 0.4975\n",
      "Epoch 115/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.8955 - loss: 0.2255 - val_accuracy: 0.8269 - val_loss: 0.4957\n",
      "Epoch 116/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9040 - loss: 0.2024 - val_accuracy: 0.8322 - val_loss: 0.5319\n",
      "Epoch 117/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.9073 - loss: 0.2079 - val_accuracy: 0.8295 - val_loss: 0.5175\n",
      "Epoch 118/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.9127 - loss: 0.2032 - val_accuracy: 0.8330 - val_loss: 0.5144\n",
      "Epoch 119/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.9065 - loss: 0.2032 - val_accuracy: 0.8375 - val_loss: 0.5167\n",
      "Epoch 120/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9032 - loss: 0.2118 - val_accuracy: 0.8269 - val_loss: 0.5170\n",
      "Epoch 121/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.9036 - loss: 0.2067 - val_accuracy: 0.8383 - val_loss: 0.5309\n",
      "Epoch 122/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.8973 - loss: 0.2104 - val_accuracy: 0.8269 - val_loss: 0.5396\n",
      "Epoch 123/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9024 - loss: 0.2071 - val_accuracy: 0.8330 - val_loss: 0.5358\n",
      "Epoch 124/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9070 - loss: 0.1963 - val_accuracy: 0.8322 - val_loss: 0.5314\n",
      "Epoch 125/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.8981 - loss: 0.2190 - val_accuracy: 0.8189 - val_loss: 0.5238\n",
      "Epoch 126/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.9142 - loss: 0.1922 - val_accuracy: 0.8313 - val_loss: 0.5294\n",
      "Epoch 127/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8984 - loss: 0.2088 - val_accuracy: 0.8322 - val_loss: 0.5179\n",
      "Epoch 128/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - accuracy: 0.9134 - loss: 0.1943 - val_accuracy: 0.8269 - val_loss: 0.5362\n",
      "Epoch 129/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9043 - loss: 0.2130 - val_accuracy: 0.8304 - val_loss: 0.5336\n",
      "Epoch 130/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9042 - loss: 0.2092 - val_accuracy: 0.8313 - val_loss: 0.5257\n",
      "Epoch 131/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9070 - loss: 0.2039 - val_accuracy: 0.8322 - val_loss: 0.5187\n",
      "Epoch 132/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.9034 - loss: 0.2079 - val_accuracy: 0.8145 - val_loss: 0.5384\n",
      "Epoch 133/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.8971 - loss: 0.2142 - val_accuracy: 0.8277 - val_loss: 0.5462\n",
      "Epoch 134/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - accuracy: 0.9043 - loss: 0.2032 - val_accuracy: 0.8260 - val_loss: 0.5410\n",
      "Epoch 135/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9058 - loss: 0.2015 - val_accuracy: 0.8295 - val_loss: 0.5529\n",
      "Epoch 136/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9080 - loss: 0.2061 - val_accuracy: 0.8198 - val_loss: 0.5518\n",
      "Epoch 137/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 903us/step - accuracy: 0.9125 - loss: 0.1952 - val_accuracy: 0.8163 - val_loss: 0.5479\n",
      "Epoch 138/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 836us/step - accuracy: 0.9069 - loss: 0.1962 - val_accuracy: 0.8198 - val_loss: 0.5636\n",
      "Epoch 139/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - accuracy: 0.9136 - loss: 0.1964 - val_accuracy: 0.8277 - val_loss: 0.5354\n",
      "Epoch 140/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.8957 - loss: 0.2043 - val_accuracy: 0.8322 - val_loss: 0.5574\n",
      "Epoch 141/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.9095 - loss: 0.1966 - val_accuracy: 0.8242 - val_loss: 0.5456\n",
      "Epoch 142/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9102 - loss: 0.2119 - val_accuracy: 0.8260 - val_loss: 0.5447\n",
      "Epoch 143/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9030 - loss: 0.2148 - val_accuracy: 0.8286 - val_loss: 0.5596\n",
      "Epoch 144/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9031 - loss: 0.2115 - val_accuracy: 0.8322 - val_loss: 0.5577\n",
      "Epoch 145/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9035 - loss: 0.2089 - val_accuracy: 0.8313 - val_loss: 0.5812\n",
      "Epoch 146/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.9156 - loss: 0.1904 - val_accuracy: 0.8092 - val_loss: 0.6834\n",
      "Epoch 147/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.8838 - loss: 0.2648 - val_accuracy: 0.8295 - val_loss: 0.5499\n",
      "Epoch 148/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - accuracy: 0.9189 - loss: 0.2000 - val_accuracy: 0.8216 - val_loss: 0.5756\n",
      "Epoch 149/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.9098 - loss: 0.1936 - val_accuracy: 0.8233 - val_loss: 0.5810\n",
      "Epoch 150/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9076 - loss: 0.1950 - val_accuracy: 0.8313 - val_loss: 0.5895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.9146 - loss: 0.1883 - val_accuracy: 0.8322 - val_loss: 0.5696\n",
      "Epoch 152/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9117 - loss: 0.2014 - val_accuracy: 0.8304 - val_loss: 0.5729\n",
      "Epoch 153/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.9133 - loss: 0.1882 - val_accuracy: 0.8216 - val_loss: 0.5910\n",
      "Epoch 154/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9166 - loss: 0.1822 - val_accuracy: 0.8286 - val_loss: 0.5932\n",
      "Epoch 155/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.9211 - loss: 0.1869 - val_accuracy: 0.8313 - val_loss: 0.5765\n",
      "Epoch 156/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 844us/step - accuracy: 0.9241 - loss: 0.1778 - val_accuracy: 0.8171 - val_loss: 0.5895\n",
      "Epoch 157/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9139 - loss: 0.1862 - val_accuracy: 0.8322 - val_loss: 0.5944\n",
      "Epoch 158/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step - accuracy: 0.9037 - loss: 0.2034 - val_accuracy: 0.8277 - val_loss: 0.5951\n",
      "Epoch 159/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9145 - loss: 0.1866 - val_accuracy: 0.8304 - val_loss: 0.5694\n",
      "Epoch 160/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.9112 - loss: 0.1831 - val_accuracy: 0.8295 - val_loss: 0.6021\n",
      "Epoch 161/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 835us/step - accuracy: 0.9138 - loss: 0.1915 - val_accuracy: 0.8286 - val_loss: 0.6003\n",
      "Epoch 162/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.9091 - loss: 0.1954 - val_accuracy: 0.8286 - val_loss: 0.6127\n",
      "Epoch 163/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.9086 - loss: 0.1892 - val_accuracy: 0.8216 - val_loss: 0.6046\n",
      "Epoch 164/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9194 - loss: 0.1824 - val_accuracy: 0.8260 - val_loss: 0.6043\n",
      "Epoch 165/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.9159 - loss: 0.1926 - val_accuracy: 0.8277 - val_loss: 0.5965\n",
      "Epoch 166/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 830us/step - accuracy: 0.9134 - loss: 0.1787 - val_accuracy: 0.8295 - val_loss: 0.6107\n",
      "Epoch 167/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9224 - loss: 0.1768 - val_accuracy: 0.8269 - val_loss: 0.6151\n",
      "Epoch 168/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.9130 - loss: 0.1862 - val_accuracy: 0.8260 - val_loss: 0.5940\n",
      "Epoch 169/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9166 - loss: 0.1846 - val_accuracy: 0.8198 - val_loss: 0.5938\n",
      "Epoch 170/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - accuracy: 0.9170 - loss: 0.1819 - val_accuracy: 0.8233 - val_loss: 0.5929\n",
      "Epoch 171/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step - accuracy: 0.9187 - loss: 0.1797 - val_accuracy: 0.8180 - val_loss: 0.5917\n",
      "Epoch 172/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step - accuracy: 0.9187 - loss: 0.1847 - val_accuracy: 0.8313 - val_loss: 0.6041\n",
      "Epoch 173/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9178 - loss: 0.1768 - val_accuracy: 0.8313 - val_loss: 0.6478\n",
      "Epoch 174/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.8978 - loss: 0.2092 - val_accuracy: 0.8224 - val_loss: 0.6183\n",
      "Epoch 175/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.9153 - loss: 0.1905 - val_accuracy: 0.8322 - val_loss: 0.6091\n",
      "Epoch 176/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 846us/step - accuracy: 0.9125 - loss: 0.1832 - val_accuracy: 0.8313 - val_loss: 0.6424\n",
      "Epoch 177/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 873us/step - accuracy: 0.9143 - loss: 0.1832 - val_accuracy: 0.8207 - val_loss: 0.6168\n",
      "Epoch 178/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 869us/step - accuracy: 0.9113 - loss: 0.1919 - val_accuracy: 0.8295 - val_loss: 0.6166\n",
      "Epoch 179/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - accuracy: 0.9215 - loss: 0.1748 - val_accuracy: 0.8286 - val_loss: 0.6293\n",
      "Epoch 180/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.9170 - loss: 0.1843 - val_accuracy: 0.8330 - val_loss: 0.6206\n",
      "Epoch 181/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - accuracy: 0.9195 - loss: 0.1814 - val_accuracy: 0.8357 - val_loss: 0.6129\n",
      "Epoch 182/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9204 - loss: 0.1852 - val_accuracy: 0.8304 - val_loss: 0.6148\n",
      "Epoch 183/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 885us/step - accuracy: 0.9040 - loss: 0.2030 - val_accuracy: 0.8260 - val_loss: 0.6242\n",
      "Epoch 184/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9184 - loss: 0.1838 - val_accuracy: 0.8366 - val_loss: 0.6252\n",
      "Epoch 185/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.9163 - loss: 0.1770 - val_accuracy: 0.8233 - val_loss: 0.6253\n",
      "Epoch 186/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - accuracy: 0.9143 - loss: 0.1880 - val_accuracy: 0.8375 - val_loss: 0.6295\n",
      "Epoch 187/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 843us/step - accuracy: 0.9171 - loss: 0.1780 - val_accuracy: 0.8207 - val_loss: 0.6338\n",
      "Epoch 188/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 858us/step - accuracy: 0.9212 - loss: 0.1709 - val_accuracy: 0.8295 - val_loss: 0.6352\n",
      "Epoch 189/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9240 - loss: 0.1780 - val_accuracy: 0.8260 - val_loss: 0.6433\n",
      "Epoch 190/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9187 - loss: 0.1773 - val_accuracy: 0.8260 - val_loss: 0.6514\n",
      "Epoch 191/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9168 - loss: 0.1827 - val_accuracy: 0.8383 - val_loss: 0.6284\n",
      "Epoch 192/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9250 - loss: 0.1653 - val_accuracy: 0.8224 - val_loss: 0.6416\n",
      "Epoch 193/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 879us/step - accuracy: 0.9141 - loss: 0.1821 - val_accuracy: 0.8171 - val_loss: 0.6320\n",
      "Epoch 194/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9144 - loss: 0.1858 - val_accuracy: 0.8171 - val_loss: 0.6716\n",
      "Epoch 195/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 855us/step - accuracy: 0.9224 - loss: 0.1682 - val_accuracy: 0.8198 - val_loss: 0.6957\n",
      "Epoch 196/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9160 - loss: 0.1925 - val_accuracy: 0.8313 - val_loss: 0.6609\n",
      "Epoch 197/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9229 - loss: 0.1800 - val_accuracy: 0.8322 - val_loss: 0.6186\n",
      "Epoch 198/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - accuracy: 0.9177 - loss: 0.1888 - val_accuracy: 0.8410 - val_loss: 0.6230\n",
      "Epoch 199/200\n",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 842us/step - accuracy: 0.9122 - loss: 0.2138 - val_accuracy: 0.8286 - val_loss: 0.6111\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/83\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - accuracy: 0.9293 - loss: 0.1723 - val_accuracy: 0.8092 - val_loss: 0.6023\n"
     ]
    }
   ],
   "source": [
    "history = fnn_model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b28dc",
   "metadata": {},
   "source": [
    "Running our model for 200 epochs has yielded us an extra 6% in accuracy, bridging the gap between our model with fewer attributes, and our model with more attributes. This means that we can have a model with similar performance and less inputs at the cost of a longer training period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "358b2b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract accuracy values from the training history\n",
    "train_accuracy = history.history['accuracy']\n",
    "\n",
    "# Plot the accuracy over epochs\n",
    "epochs = range(1, len(train_accuracy) + 1)\n",
    "plt.plot(epochs, train_accuracy, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, fit_line(epochs), 'b--', label='Line of Best Fit')\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87fcf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fnn_model.predict(X_test)\n",
    "y_pred = (y_pred >= 0.5).astype(int) # round probabilities to get binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = fnn_model.predict(X_train)\n",
    "y_pred_train = (y_pred_train >= 0.5).astype(int) # round probabilities to get binary labels\n",
    "    \n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad429ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the most frequent value\n",
    "most_frequent = (y_test == 0).sum() if (y_test == 0).sum() >= (y_test == 1).sum() else (y_test == 1).sum()\n",
    "\n",
    "null_accuracy = most_frequent/len(y_test)\n",
    "print('Null accuracy score: {0:0.4f}'. format(null_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c01cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7654dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Phishing', 'Actual Legitimate'], \n",
    "                                 index=['Predict Phishing', 'Predict Legitimate'])\n",
    "\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ac5a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[0,0]\n",
    "TN = cm[1,1]\n",
    "FP = cm[0,1]\n",
    "FN = cm[1,0]\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "recall = TP / float(TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "negative_predictive_value = TN / (TN + FN)\n",
    "accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "f1_score = 2*((precision * recall) / (precision + recall))\n",
    "balanced_accuracy = (recall + specificity) / 2\n",
    "\n",
    "print('Accuracy : {0:0.4f}'.format(accuracy))\n",
    "print('Precision : {0:0.4f}'.format(precision))\n",
    "print('Recall: {0:0.4f}'.format(recall))\n",
    "print('Specificity : {0:0.4f}'.format(specificity))\n",
    "print('NPV: {0:0.4f}'.format(negative_predictive_value))\n",
    "print('F1 Score: {0:0.4f}'.format(f1_score))\n",
    "print('Balanced Accuracy: {0:0.4f}'.format(balanced_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492cddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
